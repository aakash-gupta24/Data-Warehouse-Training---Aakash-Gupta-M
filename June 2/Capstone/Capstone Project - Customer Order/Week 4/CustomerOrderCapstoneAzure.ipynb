{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9078b162-0fd2-4ae8-95d8-1a07b02bc719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=833800893595081#setting/sparkui/0611-043338-tjd8m6e5/driver-3937486981108638751\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7be0b1f095d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"CustomerOrderCapstone\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a8e1cfd-85bf-415f-aa45-7b36d1090acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- order_id: integer (nullable = true)\n |-- customer_id: integer (nullable = true)\n |-- order_date: string (nullable = true)\n |-- delivery_date: string (nullable = true)\n |-- amount: double (nullable = true)\n |-- product_name: string (nullable = true)\n\nroot\n |-- order_id: integer (nullable = true)\n |-- current_status: string (nullable = true)\n |-- status_date: date (nullable = true)\n\nroot\n |-- customer_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone: string (nullable = true)\n |-- region: string (nullable = true)\n\n+--------+-----------+----------+-------------+------+--------------------+\n|order_id|customer_id|order_date|delivery_date|amount|        product_name|\n+--------+-----------+----------+-------------+------+--------------------+\n|       1|          1| #########|    #########|250.75|      Wireless Mouse|\n|       2|          3| #########|    #########| 89.99|Bluetooth Headphones|\n|       3|          5| #########|    #########| 120.5| External Hard Drive|\n|       4|          7| #########|    #########| 450.0|          Smartphone|\n|       5|         10| #########|    #########| 300.1|     Gaming Keyboard|\n|       6|          3| #########|    #########| 150.0|    Portable Charger|\n|       7|         20| #########|    #########|499.99|          4K Monitor|\n|       8|         15| #########|    #########| 99.95|    Wireless Earbuds|\n|       9|         15| #########|    #########| 75.45|           USB-C Hub|\n|      10|          6| #########|    #########| 199.0|          Smartwatch|\n|      11|         21| #########|    #########| 180.0|        Laptop Stand|\n|      12|         22| #########|    #########| 210.3|           Desk Lamp|\n|      13|         23| #########|    #########| 300.0|     Wireless Router|\n|      14|         25| #########|    #########| 120.0| Mechanical Keyboard|\n|      15|          1| #########|    #########|  99.0|    Gaming Mouse Pad|\n|      16|         30| #########|    #########| 85.75|           HD Webcam|\n|      17|         18| #########|    #########|  65.5|   Bluetooth Speaker|\n|      18|         33| #########|    #########| 130.2|      Graphic Tablet|\n|      19|         31| #########|    #########|  75.0|    Wireless Charger|\n|      20|         20| #########|    #########| 250.0|        External SSD|\n+--------+-----------+----------+-------------+------+--------------------+\nonly showing top 20 rows\n\n+--------+--------------+-----------+\n|order_id|current_status|status_date|\n+--------+--------------+-----------+\n|       1|     Delivered| 2025-05-06|\n|       2|    In Transit| 2025-05-07|\n|       3|       Delayed| 2025-05-08|\n|       4|     Cancelled| 2025-05-09|\n|       5|     Delivered| 2025-05-12|\n|       6|    In Transit| 2025-05-11|\n|       7|     Delivered| 2025-05-14|\n|       8|     Delivered| 2025-05-15|\n|       9|    In Transit| 2025-05-16|\n|      10|     Delivered| 2025-05-17|\n|      11|     Delivered| 2025-05-18|\n|      12|       Delayed| 2025-05-19|\n|      13|    In Transit| 2025-05-20|\n|      14|     Delivered| 2025-05-21|\n|      15|     Delivered| 2025-05-22|\n|      16|     Cancelled| 2025-05-23|\n|      17|     Delivered| 2025-05-24|\n|      18|    In Transit| 2025-05-25|\n|      19|     Delivered| 2025-05-26|\n|      20|     Delivered| 2025-05-27|\n+--------+--------------+-----------+\nonly showing top 20 rows\n\n+-----------+--------------+--------------------+--------+-------+\n|customer_id|          name|               email|   phone| region|\n+-----------+--------------+--------------------+--------+-------+\n|          1| Alice Johnson|alice.johnson@exa...|555-0101|  North|\n|          2|     Bob Smith|bob.smith@example...|555-0102|  South|\n|          3|   Carol Davis|carol.davis@examp...|555-0103|   East|\n|          4|  David Wilson|david.wilson@exam...|555-0104|   West|\n|          5|     Eva Brown|eva.brown@example...|555-0105|Central|\n|          6|   Frank Moore|frank.moore@examp...|555-0106|  North|\n|          7|     Grace Lee|grace.lee@example...|555-0107|  South|\n|          8|  Henry Walker|henry.walker@exam...|555-0108|   East|\n|          9|   Isla Harris|isla.harris@examp...|555-0109|   West|\n|         10|    Jack Young|jack.young@exampl...|555-0110|Central|\n|         11|    Karen King|karen.king@exampl...|555-0111|  North|\n|         12|    Liam Scott|liam.scott@exampl...|555-0112|  South|\n|         13|     Mia Adams|mia.adams@example...|555-0113|   East|\n|         14|    Noah Baker|noah.baker@exampl...|555-0114|   West|\n|         15|  Olivia Clark|olivia.clark@exam...|555-0115|Central|\n|         16|   Paul Carter|paul.carter@examp...|555-0116|  North|\n|         17|Quinn Mitchell|quinn.mitchell@ex...|555-0117|  South|\n|         18|  Rachel Perez|rachel.perez@exam...|555-0118|   East|\n|         19|     Sam Evans|sam.evans@example...|555-0119|   West|\n|         20|  Tina Edwards|tina.edwards@exam...|555-0120|Central|\n+-----------+--------------+--------------------+--------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#  1. Load the CSVs into PySpark\n",
    "spark.conf.set(\"fs.azure.account.key.hestore.blob.core.windows.net\",\"t9GPzv3EUQuPiAc+xYOGz8ugxHJYyeq+mZwiYW3CowXMCr4j0H0sofY2yXGapzCyksI7PYl/rUDj+ASt2AFRBQ==\")\n",
    "\n",
    "# Load orders\n",
    "orders_df=spark.read.csv(\"wasbs://customerordercapstone@hestore.blob.core.windows.net/orders.csv\",header=True,inferSchema=True)\n",
    "\n",
    "# Load delivery status\n",
    "delivery_status_df=spark.read.csv(\"wasbs://customerordercapstone@hestore.blob.core.windows.net/delivery_status.csv\",header=True,inferSchema=True)\n",
    "\n",
    "# Load customers\n",
    "customers_df=spark.read.csv(\"wasbs://customerordercapstone@hestore.blob.core.windows.net/customers.csv\",header=True,inferSchema=True)\n",
    "\n",
    "orders_df.printSchema()\n",
    "delivery_status_df.printSchema()\n",
    "customers_df.printSchema()\n",
    "orders_df.show()\n",
    "delivery_status_df.show()\n",
    "customers_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e0ad25e-e469-455a-b2a8-76f1cf683469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+-------------+------+--------------------+\n|order_id|customer_id|order_date|delivery_date|amount|        product_name|\n+--------+-----------+----------+-------------+------+--------------------+\n|      31|         11|      NULL|         NULL| 300.0|     Laptop Backpack|\n|      34|         16|      NULL|         NULL| 195.5|      Smart Home Hub|\n|      28|         34|      NULL|         NULL| 185.6|    Portable Speaker|\n|      26|         26|      NULL|         NULL| 140.4|     Bluetooth Mouse|\n|      27|         29|      NULL|         NULL| 110.0|     USB Flash Drive|\n|      44|         10|      NULL|         NULL|220.35|  Laptop Cooling Pad|\n|      12|         22|      NULL|         NULL| 210.3|           Desk Lamp|\n|      22|          7|      NULL|         NULL| 175.5|       Action Camera|\n|      47|         12|      NULL|         NULL| 225.5|      Smart Doorbell|\n|       1|          1|      NULL|         NULL|250.75|      Wireless Mouse|\n|      13|         23|      NULL|         NULL| 300.0|     Wireless Router|\n|       6|          3|      NULL|         NULL| 150.0|    Portable Charger|\n|      16|         30|      NULL|         NULL| 85.75|           HD Webcam|\n|       3|          5|      NULL|         NULL| 120.5| External Hard Drive|\n|      20|         20|      NULL|         NULL| 250.0|        External SSD|\n|      40|          2|      NULL|         NULL| 330.0|        Gaming Chair|\n|      48|         14|      NULL|         NULL| 195.0|  Fitness Smartwatch|\n|       5|         10|      NULL|         NULL| 300.1|     Gaming Keyboard|\n|      19|         31|      NULL|         NULL|  75.0|    Wireless Charger|\n|      41|          3|      NULL|         NULL| 150.0|Wireless Charging...|\n+--------+-----------+----------+-------------+------+--------------------+\nonly showing top 20 rows\n\n+--------+--------------+-----------+\n|order_id|current_status|status_date|\n+--------+--------------+-----------+\n|       1|     Delivered| 2025-05-06|\n|       2|    In Transit| 2025-05-07|\n|       3|       Delayed| 2025-05-08|\n|       4|     Cancelled| 2025-05-09|\n|       5|     Delivered| 2025-05-12|\n|       6|    In Transit| 2025-05-11|\n|       7|     Delivered| 2025-05-14|\n|       8|     Delivered| 2025-05-15|\n|       9|    In Transit| 2025-05-16|\n|      10|     Delivered| 2025-05-17|\n|      11|     Delivered| 2025-05-18|\n|      12|       Delayed| 2025-05-19|\n|      13|    In Transit| 2025-05-20|\n|      14|     Delivered| 2025-05-21|\n|      15|     Delivered| 2025-05-22|\n|      16|     Cancelled| 2025-05-23|\n|      17|     Delivered| 2025-05-24|\n|      18|    In Transit| 2025-05-25|\n|      19|     Delivered| 2025-05-26|\n|      20|     Delivered| 2025-05-27|\n+--------+--------------+-----------+\nonly showing top 20 rows\n\n+-----------+----------------+--------------------+--------+-------+\n|customer_id|            name|               email|   phone| region|\n+-----------+----------------+--------------------+--------+-------+\n|         31|     Ella Bailey|ella.bailey@examp...|555-0131|  North|\n|         65|    Mark Coleman|mark.coleman@exam...|555-0165|Central|\n|         53|   Adam Hamilton|adam.hamilton@exa...|555-0153|   East|\n|         34|Harry Richardson|harry.richardson@...|555-0134|   West|\n|         28|    Brian Morgan|brian.morgan@exam...|555-0128|   East|\n|         26|    Zachary Reed|zachary.reed@exam...|555-0126|  North|\n|         27|        Amy Cook|amy.cook@example.com|555-0127|  South|\n|         44|  Raymond Foster|raymond.foster@ex...|555-0144|   West|\n|         12|      Liam Scott|liam.scott@exampl...|555-0112|  South|\n|         22|  Victor Collins|victor.collins@ex...|555-0122|  South|\n|         47|  Ursula Russell|ursula.russell@ex...|555-0147|  South|\n|          1|   Alice Johnson|alice.johnson@exa...|555-0101|  North|\n|         52|        Zoe Ford|zoe.ford@example.com|555-0152|  South|\n|         13|       Mia Adams|mia.adams@example...|555-0113|   East|\n|          6|     Frank Moore|frank.moore@examp...|555-0106|  North|\n|         16|     Paul Carter|paul.carter@examp...|555-0116|  North|\n|          3|     Carol Davis|carol.davis@examp...|555-0103|   East|\n|         20|    Tina Edwards|tina.edwards@exam...|555-0120|Central|\n|         40|       Nate Gray|nate.gray@example...|555-0140|Central|\n|         57|      Ethan West|ethan.west@exampl...|555-0157|  South|\n+-----------+----------------+--------------------+--------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Clean and Format Data\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Format dates\n",
    "orders_df_clean = orders_df.withColumn(\"order_date\", to_date(\"order_date\", \"dd-MM-yyyy\")).withColumn(\"delivery_date\", to_date(\"delivery_date\", \"dd-MM-yyyy\"))\n",
    "\n",
    "delivery_status_df_clean = delivery_status_df.withColumn(\"status_date\", to_date(\"status_date\", \"dd-MM-yyyy\"))\n",
    "\n",
    "# Optional: drop nulls or bad rows\n",
    "orders_df_clean = orders_df_clean.dropna(subset=[\"order_id\", \"customer_id\"])\n",
    "delivery_status_df_clean = delivery_status_df_clean.dropna(subset=[\"order_id\", \"current_status\"])\n",
    "customers_df_clean = customers_df.dropna(subset=[\"customer_id\"])\n",
    "\n",
    "# 1. Cleaning and Formatting orders_df\n",
    "# Drop rows with null order_id or customer_id\n",
    "orders_df_clean = orders_df.dropna(subset=[\"order_id\", \"customer_id\"])\n",
    "\n",
    "# Convert order_date to proper DateType (assuming dd-MM-yyyy format)\n",
    "from pyspark.sql.functions import to_date\n",
    "orders_df_clean = orders_df_clean.withColumn(\"order_date\", to_date(\"order_date\", \"dd-MM-yyyy\"))\n",
    "\n",
    "# Convert delivery_date to proper DateType\n",
    "orders_df_clean = orders_df_clean.withColumn(\"delivery_date\", to_date(\"delivery_date\", \"dd-MM-yyyy\"))\n",
    "\n",
    "# Remove rows with negative or zero amount\n",
    "orders_df_clean = orders_df_clean.filter(orders_df_clean[\"amount\"] > 0)\n",
    "\n",
    "# Optional: Drop duplicates\n",
    "orders_df_clean = orders_df_clean.dropDuplicates([\"order_id\"])\n",
    "\n",
    "# Show cleaned orders\n",
    "orders_df_clean.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Cleaning and Formatting delivery_status_df\n",
    "# Drop rows with null order_id or current_status\n",
    "delivery_status_df_clean = delivery_status_df.dropna(subset=[\"order_id\", \"current_status\"])\n",
    "\n",
    "# Format status_date to DateType\n",
    "delivery_status_df_clean = delivery_status_df_clean.withColumn(\"status_date\", to_date(\"status_date\", \"dd-MM-yyyy\"))\n",
    "\n",
    "# Optional: Remove duplicate entries, keeping the latest status\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc\n",
    "\n",
    "window_spec = Window.partitionBy(\"order_id\").orderBy(desc(\"status_date\"))\n",
    "delivery_status_df_clean = delivery_status_df_clean.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "delivery_status_df_clean = delivery_status_df_clean.filter(\"row_num = 1\").drop(\"row_num\")\n",
    "\n",
    "# Show cleaned delivery status\n",
    "delivery_status_df_clean.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Cleaning and Formatting customers_df\n",
    "# Drop rows with null customer_id or email\n",
    "customers_df_clean = customers_df.dropna(subset=[\"customer_id\", \"email\"])\n",
    "\n",
    "# Remove duplicate customers (keep first occurrence)\n",
    "customers_df_clean = customers_df_clean.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "# Trim whitespaces from name and email\n",
    "from pyspark.sql.functions import trim\n",
    "\n",
    "customers_df_clean = customers_df_clean.withColumn(\"name\", trim(customers_df_clean[\"name\"]))\n",
    "customers_df_clean = customers_df_clean.withColumn(\"email\", trim(customers_df_clean[\"email\"]))\n",
    "\n",
    "# Optional: lowercase the email for consistency\n",
    "from pyspark.sql.functions import lower\n",
    "\n",
    "customers_df_clean = customers_df_clean.withColumn(\"email\", lower(customers_df_clean[\"email\"]))\n",
    "\n",
    "# Show cleaned customers\n",
    "customers_df_clean.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9fa9d7a-3b4f-43d3-b4d4-41c4abc7c514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+-------------+------+--------------------+--------------+-----------+\n|order_id|customer_id|order_date|delivery_date|amount|        product_name|current_status|status_date|\n+--------+-----------+----------+-------------+------+--------------------+--------------+-----------+\n|      31|         11|      NULL|         NULL| 300.0|     Laptop Backpack|       Delayed| 2025-06-07|\n|      34|         16|      NULL|         NULL| 195.5|      Smart Home Hub|    In Transit| 2025-06-10|\n|      28|         34|      NULL|         NULL| 185.6|    Portable Speaker|    In Transit| 2025-06-04|\n|      26|         26|      NULL|         NULL| 140.4|     Bluetooth Mouse|     Cancelled| 2025-06-02|\n|      27|         29|      NULL|         NULL| 110.0|     USB Flash Drive|     Delivered| 2025-06-03|\n|      44|         10|      NULL|         NULL|220.35|  Laptop Cooling Pad|       Delayed| 2025-06-20|\n|      12|         22|      NULL|         NULL| 210.3|           Desk Lamp|       Delayed| 2025-05-19|\n|      22|          7|      NULL|         NULL| 175.5|       Action Camera|     Delivered| 2025-05-29|\n|      47|         12|      NULL|         NULL| 225.5|      Smart Doorbell|     Delivered| 2025-06-23|\n|       1|          1|      NULL|         NULL|250.75|      Wireless Mouse|     Delivered| 2025-05-06|\n|      13|         23|      NULL|         NULL| 300.0|     Wireless Router|    In Transit| 2025-05-20|\n|       6|          3|      NULL|         NULL| 150.0|    Portable Charger|    In Transit| 2025-05-11|\n|      16|         30|      NULL|         NULL| 85.75|           HD Webcam|     Cancelled| 2025-05-23|\n|       3|          5|      NULL|         NULL| 120.5| External Hard Drive|       Delayed| 2025-05-08|\n|      20|         20|      NULL|         NULL| 250.0|        External SSD|     Delivered| 2025-05-27|\n|      40|          2|      NULL|         NULL| 330.0|        Gaming Chair|     Delivered| 2025-06-16|\n|      48|         14|      NULL|         NULL| 195.0|  Fitness Smartwatch|     Cancelled| 2025-06-24|\n|       5|         10|      NULL|         NULL| 300.1|     Gaming Keyboard|     Delivered| 2025-05-12|\n|      19|         31|      NULL|         NULL|  75.0|    Wireless Charger|     Delivered| 2025-05-26|\n|      41|          3|      NULL|         NULL| 150.0|Wireless Charging...|    In Transit| 2025-06-17|\n+--------+-----------+----------+-------------+------+--------------------+--------------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#  3. Join Orders with Latest Delivery Status\n",
    "# Join orders with delivery status\n",
    "orders_with_status_df = orders_df_clean.join(delivery_status_df_clean, on=\"order_id\", how=\"left\")\n",
    "orders_with_status_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae770752-aac2-4fca-803b-87dd75c0c8c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  4. Save Result as Delta or CSV\n",
    "# Save as Delta\n",
    "orders_with_status_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/orders_with_status_delta\")\n",
    "\n",
    "# Save as CSV\n",
    "orders_with_status_df.write.option(\"header\", True).mode(\"overwrite\").csv(\"/tmp/orders_with_status_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e207d30-9218-4319-adf5-866eb5d88aaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Register as Temp Views\n",
    "orders_with_status_df.createOrReplaceTempView(\"orders_status\")\n",
    "customers_df_clean.createOrReplaceTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3244a7b-2c6b-4957-8b8f-2cf9530a1342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+--------------+\n|customer_id|          name|               email|delayed_orders|\n+-----------+--------------+--------------------+--------------+\n|          5|     Eva Brown|eva.brown@example...|             1|\n|          6|   Frank Moore|frank.moore@examp...|             1|\n|         10|    Jack Young|jack.young@exampl...|             1|\n|         22|Victor Collins|victor.collins@ex...|             1|\n|         11|    Karen King|karen.king@exampl...|             1|\n+-----------+--------------+--------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 6. SQL Query: Top 5 Delayed Customers\n",
    "spark.sql(\"\"\"select c.customer_id, c.name, c.email, count(*) as delayed_orders from orders_status o join customers c on o.customer_id = c.customer_id where o.current_status = 'Delayed' group by c.customer_id, c.name, c.email order by delayed_orders desc limit 5\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd688a87-9845-46c9-acf1-60e72f64c094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n| region|total_revenue|\n+-------+-------------+\n|Central|      2588.64|\n|  South|       2082.3|\n|   East|      1947.59|\n|  North|       1885.1|\n|   West|      1496.05|\n+-------+-------------+\n\n+--------------+------------+\n|current_status|total_orders|\n+--------------+------------+\n|     Delivered|          28|\n|    In Transit|          12|\n|     Cancelled|           5|\n|       Delayed|           5|\n+--------------+------------+\n\n+-----------+-------------+-----------+\n|customer_id|         name|total_spent|\n+-----------+-------------+-----------+\n|         20| Tina Edwards|    1185.79|\n|          7|    Grace Lee|      625.5|\n|          8| Henry Walker|     555.75|\n|          1|Alice Johnson|     539.95|\n|         10|   Jack Young|     520.45|\n+-----------+-------------+-----------+\n\n+--------+-------------+-----------+-----------+\n|order_id|delivery_date|status_date|customer_id|\n+--------+-------------+-----------+-----------+\n+--------+-------------+-----------+-----------+\n\n+--------------------+--------------+\n|        product_name|average_amount|\n+--------------------+--------------+\n|          Smartphone|         450.0|\n|          4K Monitor|         355.0|\n|        Gaming Chair|         330.0|\n|Noise Cancelling ...|         320.0|\n|       Gaming Laptop|         315.4|\n|   Smartphone Gimbal|        305.45|\n|     Gaming Keyboard|         300.1|\n|     Wireless Router|         300.0|\n|     Laptop Backpack|         300.0|\n|    4K Action Camera|         280.0|\n|      Wireless Mouse|        250.75|\n|        External SSD|         250.0|\n|  Portable Projector|         245.0|\n|Mechanical Gaming...|         225.8|\n|   Wireless Keyboard|        225.75|\n|      Smart Doorbell|         225.5|\n|  Laptop Cooling Pad|        220.35|\n|Noise Cancelling ...|        210.75|\n|           Desk Lamp|         210.3|\n|    Smart Thermostat|         210.0|\n+--------------------+--------------+\nonly showing top 20 rows\n\n+-----------------+------------+\n|     product_name|total_orders|\n+-----------------+------------+\n|       Smartwatch|           2|\n| Wireless Earbuds|           2|\n|Bluetooth Speaker|           2|\n|       4K Monitor|           2|\n|   Wireless Mouse|           1|\n+-----------------+------------+\n\n+-----------+----+--------------+\n|customer_id|name|delayed_orders|\n+-----------+----+--------------+\n+-----------+----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "#more sql queries\n",
    "# 1. Total revenue per region\n",
    "spark.sql(\"\"\"select c.region, round(sum(o.amount), 2) as total_revenue from orders_status o join customers c on o.customer_id = c.customer_id group by c.region order by total_revenue desc\"\"\").show()\n",
    "\n",
    "#  2. Count of orders by delivery status\n",
    "spark.sql(\"\"\"select current_status, count(*) as total_orders from orders_status group by current_status order by total_orders desc\"\"\").show()\n",
    "\n",
    "# 3. Top 5 customers by total spending\n",
    "spark.sql(\"\"\"select c.customer_id, c.name, round(sum(o.amount), 2) as total_spent from orders_status o join customers c on o.customer_id = c.customer_id group by c.customer_id, c.name order by total_spent desc limit 5\"\"\").show()\n",
    "\n",
    "# 4. Orders delivered late (delivery_date after status_date)\n",
    "spark.sql(\"\"\"select o.order_id, o.delivery_date, o.status_date, o.customer_id from orders_status o where o.current_status = 'delivered' and o.delivery_date > o.status_date\"\"\").show()\n",
    "\n",
    "# 5. Average order amount per product\n",
    "spark.sql(\"\"\"select product_name, round(avg(amount), 2) as average_amount from orders_status group by product_name order by average_amount desc\"\"\").show()\n",
    "\n",
    "# 6. Most purchased products (by order count)\n",
    "spark.sql(\"\"\"select product_name, count(*) as total_orders from orders_status group by product_name order by total_orders desc limit 5\"\"\").show()\n",
    "\n",
    "# 7. Customers who placed more than 1 delayed order\n",
    "spark.sql(\"\"\"select c.customer_id, c.name, count(*) as delayed_orders from orders_status o join customers c on o.customer_id = c.customer_id where o.current_status = 'delayed' group by c.customer_id, c.name having count(*) > 1\"\"\").show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CustomerOrderCapstone",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}