{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a06492e-8f36-415b-84ee-a41b94533f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=833800893595081#setting/sparkui/0611-043338-tjd8m6e5/driver-6941113433127875465\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x70994e8ad610>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "saprk=SparkSession.builder.appName(\"SupplyChainMonitoringCapstone\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe79b900-ed64-4508-a9b1-0ba52a965375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- inventory_id: integer (nullable = true)\n |-- item_name: string (nullable = true)\n |-- quantity: integer (nullable = true)\n |-- price: double (nullable = true)\n |-- supplier_id: integer (nullable = true)\n\nroot\n |-- supplier_id: integer (nullable = true)\n |-- supplier_name: string (nullable = true)\n |-- contact_email: string (nullable = true)\n\nroot\n |-- order_id: integer (nullable = true)\n |-- order_date: string (nullable = true)\n |-- supplier_id: integer (nullable = true)\n |-- status: string (nullable = true)\n |-- quantity: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Read the CSV Files using PySpark\n",
    "spark.conf.set(\"fs.azure.account.key.hestore.blob.core.windows.net\",\"t9GPzv3EUQuPiAc+xYOGz8ugxHJYyeq+mZwiYW3CowXMCr4j0H0sofY2yXGapzCyksI7PYl/rUDj+ASt2AFRBQ==\")\n",
    "\n",
    "# Read inventory.csv\n",
    "inventory_df=spark.read.csv(\"wasbs://supplychaincapstone@hestore.blob.core.windows.net/inventory.csv\",header=True,inferSchema=True)\n",
    "\n",
    "# Read suppliers.csv\n",
    "suppliers_df=spark.read.csv(\"wasbs://supplychaincapstone@hestore.blob.core.windows.net/suppliers.csv\",header=True,inferSchema=True)\n",
    "\n",
    "# Read orders.csv\n",
    "orders_df=spark.read.csv(\"wasbs://supplychaincapstone@hestore.blob.core.windows.net/orders.csv\",header=True,inferSchema=True)\n",
    "\n",
    "inventory_df.printSchema()\n",
    "suppliers_df.printSchema()\n",
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32f2aa8-90b9-4898-8b49-dfa90db87c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#clean and filter data\n",
    "# Drop rows with nulls\n",
    "inventory_df_clean = inventory_df.dropna()\n",
    "suppliers_df_clean = suppliers_df.dropna()\n",
    "orders_df_clean = orders_df.dropna()\n",
    "\n",
    "# Filter orders with valid supplier_id\n",
    "valid_supplier_ids = [row['supplier_id'] for row in suppliers_df.select(\"supplier_id\").collect()]\n",
    "orders_df_clean = orders_df_clean.filter(orders_df_clean['supplier_id'].isin(valid_supplier_ids))\n",
    "\n",
    "# On inventory_df\n",
    "#1. Remove duplicate rows\n",
    "inventory_df_clean = inventory_df.dropDuplicates()\n",
    "\n",
    "# 2. Filter items with zero or negative quantity/price\n",
    "inventory_df_clean = inventory_df_clean.filter(\"quantity > 0 AND price > 0\")\n",
    "\n",
    "# 3. Standardize column names (lowercase, no spaces)\n",
    "for col in inventory_df_clean.columns:\n",
    "    inventory_df_clean = inventory_df_clean.withColumnRenamed(col, col.lower().replace(\" \", \"_\"))\n",
    "\n",
    "# 4. Detect missing supplier_ids (foreign key check)\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "valid_supplier_ids_df = suppliers_df.select(\"supplier_id\").distinct()\n",
    "inventory_df_clean = inventory_df_clean.join(valid_supplier_ids_df, on=\"supplier_id\", how=\"inner\")\n",
    "\n",
    "\n",
    "# On suppliers_df\n",
    "# 1. Trim and standardize text fields\n",
    "from pyspark.sql.functions import trim, lower\n",
    "\n",
    "suppliers_df_clean = suppliers_df.withColumn(\"supplier_name\", trim(lower(col(\"supplier_name\"))))\n",
    "\n",
    "# 2. Validate emails\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Extract email pattern; valid if match is non-empty\n",
    "suppliers_df_clean = suppliers_df_clean.withColumn(\n",
    "    \"valid_email\",\n",
    "    regexp_extract(col(\"contact_email\"), r\"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\", 0)\n",
    ").filter(\"valid_email != ''\")\n",
    "\n",
    "\n",
    "# On orders_df\n",
    "# 1. Remove future dates or invalid formats\n",
    "from pyspark.sql.functions import to_date, current_date\n",
    "\n",
    "orders_df_clean = orders_df.withColumn(\"order_date\", to_date(\"order_date\"))\n",
    "orders_df_clean = orders_df_clean.filter(col(\"order_date\") <= current_date())\n",
    "\n",
    "# 2. Filter for only allowed statuses\n",
    "valid_statuses = ['pending', 'shipped', 'delivered']\n",
    "orders_df_clean = orders_df_clean.filter(col(\"status\").isin(valid_statuses))\n",
    "\n",
    "# 3. Remove orders with unrealistic quantities (e.g., negative or too high)\n",
    "orders_df_clean = orders_df_clean.filter((col(\"quantity\") > 0) & (col(\"quantity\") <= 1000))\n",
    "\n",
    "# 4. Remove orders with missing/invalid supplier IDs\n",
    "orders_df_clean = orders_df_clean.join(valid_supplier_ids_df, on=\"supplier_id\", how=\"inner\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b928fb20-b2da-4c56-a8fd-dd8d655803d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Save Cleaned Data as Delta or CSV\n",
    "#delta\n",
    "inventory_df_clean.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/inventory_clean_delta\")\n",
    "suppliers_df_clean.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/suppliers_clean_delta\")\n",
    "orders_df_clean.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/orders_clean_delta\")\n",
    "\n",
    "#csv\n",
    "inventory_df_clean.write.option(\"header\", True).mode(\"overwrite\").csv(\"/tmp/inventory_clean_csv\")\n",
    "suppliers_df_clean.write.option(\"header\", True).mode(\"overwrite\").csv(\"/tmp/suppliers_clean_csv\")\n",
    "orders_df_clean.write.option(\"header\", True).mode(\"overwrite\").csv(\"/tmp/orders_clean_csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d08c045-8d8f-46fc-9ce4-a571f5414ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n|total_inventory_quantity|\n+------------------------+\n|                    3725|\n+------------------------+\n\n+--------+------+-------------+\n|order_id|status|supplier_name|\n+--------+------+-------------+\n+--------+------+-------------+\n\n+------+-----------+\n|status|order_count|\n+------+-----------+\n+------+-----------+\n\n+------------------------+\n|total_inventory_quantity|\n+------------------------+\n|                    3725|\n+------------------------+\n\n+---------+--------+-----+-----------+\n|item_name|quantity|price|total_value|\n+---------+--------+-----+-----------+\n|  Item 13|     160| 16.0|     2560.0|\n|   Item 5|     200| 20.0|     4000.0|\n|  Item 16|     100| 10.0|     1000.0|\n|  Item 29|     100| 10.0|     1000.0|\n|   Item 7|     170| 17.0|     2890.0|\n|  Item 26|     135| 13.5|     1822.5|\n|  Item 14|      95|  9.5|      902.5|\n|  Item 19|      85|  8.5|      722.5|\n|  Item 18|     115| 11.5|     1322.5|\n|   Item 9|     110| 11.0|     1210.0|\n|   Item 2|     150| 15.0|     2250.0|\n|  Item 12|     140| 14.0|     1960.0|\n|  Item 20|     190| 19.0|     3610.0|\n|   Item 3|     120| 12.0|     1440.0|\n|   Item 8|      90|  9.0|      810.0|\n|   Item 6|      60|  6.0|      360.0|\n|  Item 11|      75|  7.5|      562.5|\n|   Item 1|     100| 10.0|     1000.0|\n|  Item 10|     130| 13.0|     1690.0|\n|  Item 23|     145| 14.5|     2102.5|\n+---------+--------+-----+-----------+\nonly showing top 20 rows\n\n+-------------+---------------------+\n|supplier_name|total_inventory_value|\n+-------------+---------------------+\n|   supplier y|               2402.5|\n|  supplier ab|                810.0|\n|   supplier i|               1210.0|\n|   supplier t|               3610.0|\n|   supplier c|               1440.0|\n|  supplier ad|               1210.0|\n|   supplier s|                722.5|\n|   supplier q|               1102.5|\n|   supplier w|               2102.5|\n|   supplier g|               2890.0|\n|   supplier v|               1690.0|\n|   supplier u|               1562.5|\n|   supplier o|               3240.0|\n|   supplier f|                360.0|\n|   supplier z|               1822.5|\n|   supplier k|                562.5|\n|   supplier n|                902.5|\n|   supplier d|                640.0|\n|   supplier p|               1000.0|\n|   supplier r|               1322.5|\n+-------------+---------------------+\nonly showing top 20 rows\n\n+-------------+------------+\n|supplier_name|total_orders|\n+-------------+------------+\n+-------------+------------+\n\n+------+------------+\n|status|status_count|\n+------+------------+\n+------+------------+\n\n+-------------+------------------+\n|supplier_name|avg_order_quantity|\n+-------------+------------------+\n+-------------+------------------+\n\n+-----------+--------+----------+------+--------+\n|supplier_id|order_id|order_date|status|quantity|\n+-----------+--------+----------+------+--------+\n+-----------+--------+----------+------+--------+\n\n+-----------+-------------+-------------+-----------+\n|supplier_id|supplier_name|contact_email|valid_email|\n+-----------+-------------+-------------+-----------+\n+-----------+-------------+-------------+-----------+\n\n+-----------+-------------+--------------------+--------------------+\n|supplier_id|supplier_name|       contact_email|         valid_email|\n+-----------+-------------+--------------------+--------------------+\n|          1|   supplier a|supplierA@example...|supplierA@example...|\n|          2|   supplier b|supplierB@example...|supplierB@example...|\n|          3|   supplier c|supplierC@example...|supplierC@example...|\n|          4|   supplier d|supplierD@example...|supplierD@example...|\n|          5|   supplier e|supplierE@example...|supplierE@example...|\n|          6|   supplier f|supplierF@example...|supplierF@example...|\n|          7|   supplier g|supplierG@example...|supplierG@example...|\n|          8|   supplier h|supplierH@example...|supplierH@example...|\n|          9|   supplier i|supplierI@example...|supplierI@example...|\n|         10|   supplier j|supplierJ@example...|supplierJ@example...|\n|         11|   supplier k|supplierK@example...|supplierK@example...|\n|         12|   supplier l|supplierL@example...|supplierL@example...|\n|         13|   supplier m|supplierM@example...|supplierM@example...|\n|         14|   supplier n|supplierN@example...|supplierN@example...|\n|         15|   supplier o|supplierO@example...|supplierO@example...|\n|         16|   supplier p|supplierP@example...|supplierP@example...|\n|         17|   supplier q|supplierQ@example...|supplierQ@example...|\n|         18|   supplier r|supplierR@example...|supplierR@example...|\n|         19|   supplier s|supplierS@example...|supplierS@example...|\n|         20|   supplier t|supplierT@example...|supplierT@example...|\n+-----------+-------------+--------------------+--------------------+\nonly showing top 20 rows\n\n+-------------+----------------------+\n|supplier_name|total_ordered_quantity|\n+-------------+----------------------+\n+-------------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Register as Temp Views\n",
    "inventory_df_clean.createOrReplaceTempView(\"inventory\")\n",
    "suppliers_df_clean.createOrReplaceTempView(\"suppliers\")\n",
    "orders_df_clean.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "# total quantity of all inventory\n",
    "saprk.sql(\"select sum(quantity) as total_inventory_quantity from inventory\").show()\n",
    "\n",
    "# join orders with supplier info\n",
    "spark.sql(\"select o.order_id, o.status, s.supplier_name from orders o join suppliers s on o.supplier_id = s.supplier_id\").show()\n",
    "\n",
    "# count of orders per status\n",
    "spark.sql(\"select status, count(*) as order_count from orders group by status\").show()\n",
    "\n",
    "# 1. total quantity of all inventory\n",
    "spark.sql(\"\"\"select sum(quantity) as total_inventory_quantity from inventory\"\"\").show()\n",
    "\n",
    "# 2. total inventory value per item\n",
    "spark.sql(\"\"\"select item_name, quantity, price, (quantity * price) as total_value from inventory\"\"\").show()\n",
    "\n",
    "# 3. total value of inventory per supplier\n",
    "spark.sql(\"\"\"select s.supplier_name, sum(i.quantity * i.price) as total_inventory_value from inventory i join suppliers s on i.supplier_id = s.supplier_id group by s.supplier_name\"\"\").show()\n",
    "\n",
    "# 4. number of orders per supplier\n",
    "spark.sql(\"\"\"select s.supplier_name, count(o.order_id) as total_orders from orders o join suppliers s on o.supplier_id = s.supplier_id group by s.supplier_name order by total_orders desc\"\"\").show()\n",
    "\n",
    "# 5. number of orders by status\n",
    "spark.sql(\"\"\"select status, count(*) as status_count from orders group by status\"\"\").show()\n",
    "\n",
    "# 6. average quantity per order by supplier\n",
    "spark.sql(\"\"\"select s.supplier_name, round(avg(o.quantity), 2) as avg_order_quantity from orders o join suppliers s on o.supplier_id = s.supplier_id group by s.supplier_name\"\"\").show()\n",
    "\n",
    "# 7. orders placed in the last 30 days\n",
    "spark.sql(\"\"\"select * from orders where order_date >= date_sub(current_date(), 30)\"\"\").show()\n",
    "\n",
    "# 8. suppliers without inventory items\n",
    "spark.sql(\"\"\"select s.* from suppliers s left join inventory i on s.supplier_id = i.supplier_id where i.supplier_id is null\"\"\").show()\n",
    "\n",
    "# 9. suppliers who have never delivered any order\n",
    "spark.sql(\"\"\"select s.* from suppliers s left join ( select distinct supplier_id from orders where status = 'delivered' ) d on s.supplier_id = d.supplier_id where d.supplier_id is null\"\"\").show()\n",
    "\n",
    "# 10. total quantity ordered per supplier (all statuses)\n",
    "spark.sql(\"\"\"select s.supplier_name, sum(o.quantity) as total_ordered_quantity from orders o join suppliers s on o.supplier_id = s.supplier_id group by s.supplier_name order by total_ordered_quantity desc\"\"\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SupplyChainMonitoringCapstone",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}