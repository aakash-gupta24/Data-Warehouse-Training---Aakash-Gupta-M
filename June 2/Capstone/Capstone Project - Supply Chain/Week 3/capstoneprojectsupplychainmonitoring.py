# -*- coding: utf-8 -*-
"""CapstoneProjectSupplyChainMonitoring.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xKqgQIPFPafAHqZkTmrFoqtltFQV3lAG
"""

from pyspark.sql import SparkSession

spark= SparkSession.builder.appName("CapstoneProjectSupplyChainMonitoring").getOrCreate()

spark

#Capstone Tasks:
#Load order data from CSV using PySpark orders, suppliers, inventory.
orders_df = spark.read.csv("orders.csv", header=True, inferSchema=True)
suppliers_df = spark.read.csv("suppliers.csv", header=True, inferSchema=True)
inventory_df = spark.read.csv("inventory.csv", header=True, inferSchema=True)
orders_df.show()
suppliers_df.show()
inventory_df.show()
#Filter delayed shipments.
#->pending is delayed
#->shipped is in process
#->delivered is completed
delayed_shipments_df = orders_df.filter(orders_df["status"] == "pending")
delayed_shipments_df.show()
#Group by supplier and count delayed orders.
delayed_orders_by_supplier_df = delayed_shipments_df.groupBy("supplier_id").count()
delayed_orders_by_supplier_df.show()
#Save processed data to CSV or Parquet.
delayed_shipments_df.write.mode('overwrite').csv("delayed_shipments.csv", header=True)
delayed_orders_by_supplier_df.write.mode('overwrite').csv("delayed_orders_by_supplier.csv", header=True)

#  Deliverables:
#  PySpark script to load, process, and save supply chain data
#  Output le showing grouped results

spark.stop()