# -*- coding: utf-8 -*-
"""Assignment 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o8VjZ55-8Rq4i_cY3nJLulkpcvt5suXF
"""

from pyspark.sql import SparkSession
from pyspark.sql import SparkSession
from pyspark.sql.functions import when, year, sum as _sum

spark=SparkSession.builder.appName("June9Assignment1").getOrCreate()
spark

#Load both CSV files with schema inference.
df_spark_customers = spark.read.csv("/content/customers.csv", header=True, inferSchema=True)
df_spark_orders = spark.read.csv("/content/orders.csv", header=True, inferSchema=True)
df_spark_customers.show()
df_spark_orders.show()
#List all columns and data types.
df_spark_customers.printSchema()
df_spark_orders.printSchema()
#Count the total number of customers and orders.
print(df_spark_customers.count())
print(df_spark_orders.count())
#Show distinct cities.
df_spark_customers.select("City").distinct().show()

#Add a column TotalAmount = Price * Quantity .
df_spark_orders = df_spark_orders.withColumn("TotalAmount", df_spark_orders.Price * df_spark_orders.Quantity)
df_spark_orders.show()
#Create a new column OrderYear from OrderDate .
df_spark_orders = df_spark_orders.withColumn("OrderYear", year(df_spark_orders.OrderDate))
df_spark_orders.show()
#Filter orders with TotalAmount > 10,000 .
df_spark_orders.filter(df_spark_orders.TotalAmount > 10000).show()
#Drop the Email column from customers .
df_spark_customers.drop("Email").show()

#Simulate a null in City and fill it with “Unknown”.
df_spark_customers = df_spark_customers.withColumn("City", when(df_spark_customers.City.isNull(), "Unknown").otherwise(df_spark_customers.City))
df_spark_customers.show()
#Label customers as “Loyal” if SignupDate is before 2022, else “New”.
df_spark_customers = df_spark_customers.withColumn("CustomerType", when(df_spark_customers.SignupDate < "2022-01-01", "Loyal").otherwise("New"))
df_spark_customers.show()
#Create OrderType column: "Low" if < 5,000, "High" if ≥ 5,000.
df_spark_orders = df_spark_orders.withColumn("OrderType", when(df_spark_orders.TotalAmount < 5000, "Low").otherwise("High"))
df_spark_orders.show()

#Join customers and orders on CustomerID .
df_spark_customers_orders = df_spark_customers.join(df_spark_orders, "CustomerID", "inner")
df_spark_customers_orders.show()
#Get total orders and revenue per city.
df_spark_customers_orders.groupBy("City").agg({"OrderID": "count", "TotalAmount": "sum"}).show()
#Show top 3 customers by total spend.
df_spark_customers_orders.select("CustomerID", "TotalAmount").groupBy("CustomerID").agg(_sum("TotalAmount").alias("TotalSpent")).orderBy("TotalSpent", ascending=False).show(3)
#Count how many products each category has sold.
df_spark_customers_orders.groupBy("Category").agg({"Quantity": "sum"}).show()

# Create database sales and switch to it.
spark.sql("create database if not exists sales")
spark.sql("use sales")
# Save both datasets as tables in the sales database.
# Overwrite the 'customers' table in the database
df_spark_customers.write.mode('overwrite').saveAsTable("customers")
df_spark_orders.write.mode('overwrite').saveAsTable("orders")
# Write SQL to:
# List all orders by customers from “Delhi”.
spark.sql("select * from orders where CustomerID in (select CustomerID from customers where City = 'Delhi')").show()
# Find average order value in each category.
spark.sql("select Category, avg(TotalAmount) as AvgOrderValue from orders group by Category").show()
# Create a view monthly_orders with month-wise total amount.
spark.sql("create or replace view monthly_orders as select month(OrderDate) as Month, sum(TotalAmount) as TotalAmount from orders group by month(OrderDate)")
spark.sql("select * from monthly_orders").show()

#Mask emails using regex (e.g a***@gmail.com ).
from pyspark.sql.functions import udf
from pyspark.sql.functions import col
from pyspark.sql.types import StringType
def mask_email(email):
    parts = email.split('@')
    if len(parts) == 2:
        username, domain = parts
        masked_username = username[0] + '*' * (len(username) - 2) + username[-1]
        return f"{masked_username}@{domain}"
    else:
        return email
mask_email_udf = udf(mask_email, StringType())
customers_df = df_spark_customers.withColumn("Email", mask_email_udf(col("Email")))
customers_df.show()
#Concatenate Name and City as “Name from City”.
from pyspark.sql.functions import concat, col, lit
df_spark_customers = df_spark_customers.withColumn("NameFromCity", concat(col("Name"), lit(" from "), col("City")))
df_spark_customers.show()
#Use datediff() to calculate customer age in days.
from pyspark.sql.functions import datediff,current_date
df_spark_customers = df_spark_customers.withColumn("AgeInDays", datediff(current_date(), col("SignupDate")))
df_spark_customers.show()
#Extract month name from OrderDate .
from pyspark.sql.functions import month, date_format
df_spark_orders = df_spark_orders.withColumn("MonthName", date_format(col("OrderDate"), "MMMM"))
df_spark_orders.show()

#Write a UDF to tag customers:
#“Gold” if spend > 50K, “Silver” if 10K–50K, “Bronze” if <10K.
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
def tag_customer(total_amount):
    if total_amount > 50000:
        return "Gold"
    elif total_amount >= 10000:
        return "Silver"
    else:
        return "Bronze"
tag_customer_udf = udf(tag_customer, StringType())
df_spark_customers_orders = df_spark_customers_orders.withColumn("CustomerTag", tag_customer_udf(col("TotalAmount")))
df_spark_customers_orders.show()
#Write a UDF to shorten product names (first 3 letters + ...).
def shorten_product_name(product_name):
    return product_name[:3] + "..."
shorten_product_name_udf = udf(shorten_product_name, StringType())
df_spark_customers_orders = df_spark_customers_orders.withColumn("Product", shorten_product_name_udf(col("Product")))
df_spark_customers_orders.show()

#Save the joined result as a Parquet file.
df_spark_customers_orders.write.mode('overwrite').parquet("/content/customers_orders.parquet")
#Read it back and verify schema.
df_spark_customers_orders_parquet = spark.read.parquet("/content/customers_orders.parquet")
df_spark_customers_orders_parquet.printSchema()
#Create and query a global temp view.
df_spark_customers_orders_parquet.createOrReplaceGlobalTempView("customers_orders_parquet")
spark.sql("select * from global_temp.customers_orders_parquet").show()
#Compare performance between CSV read and Parquet read.
df_spark_customers_orders_csv = spark.read.csv("/content/customers.csv", header=True, inferSchema=True)
df_spark_customers_orders_csv.show()
df_spark_customers_orders_csv.printSchema()
df_spark_customers_orders_csv.createOrReplaceGlobalTempView("customers_orders_csv")
spark.sql("select * from global_temp.customers_orders_csv").show()

#spark.sql("drop database sales")

spark.stop()