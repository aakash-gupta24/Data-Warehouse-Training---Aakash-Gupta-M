# -*- coding: utf-8 -*-
"""Assignment 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BI_WkMIBPwbqr9mpoXTeVlKOwk-vBsAf
"""

from pyspark.sql import SparkSession

spark=SparkSession.builder.appName("June9Assignment2").getOrCreate()
spark

#Read all 3 files (CSV + JSON) using PySpark.
df_employees=spark.read.csv("employees.csv",header=True,inferSchema=True)
df_employees.show()
df_attendance=spark.read.csv("attendance.csv",header=True,inferSchema=True)
df_attendance.show()
df_bonuses=spark.read.json("bonuses.json")
df_bonuses.show()
#Show schemas and sample records.
df_employees.printSchema()
df_attendance.printSchema()
df_bonuses.printSchema()
#Count distinct departments.
df_employees.select("department").distinct().count()

from pyspark.sql import functions as F
from pyspark.sql.functions import datediff, current_date, round

#Add a column TenureYears using datediff() and round().
df_employees = df_employees.withColumn("TenureYears", round(datediff(current_date(), df_employees.JoinDate) / 365, 1))
df_employees.show()
#Calculate TotalCompensation = Salary + Bonus.
df_employees_with_bonus = df_employees.join(df_bonuses, on="EmpID", how="left")
df_employees= df_employees_with_bonus.withColumn("TotalCompensation", df_employees_with_bonus.Salary + df_employees_with_bonus.Bonus)
df_employees.show()
#Filter employees with more than 2 years in the company.
df_employees.filter(df_employees_with_bonus.TenureYears > 2).show()
#Show employees who report to a manager (ManagerID is not null).
df_employees.filter(df_employees_with_bonus.ManagerID.isNotNull()).show()

#Average salary per department.
df_employees.groupBy("Department").agg(F.avg("Salary")).show()
#Number of employees under each manager.
df_employees.groupBy("ManagerID").count().show()
#Count of absences per employee.
df_attendance.groupBy("EmpID").agg(F.count(F.when(df_attendance.Status == "Absent", 1).otherwise(None)).alias("AbsentCount")).show()

#Join employees and attendance → Get attendance % (Present days / Total days).
from pyspark.sql import functions as F
from pyspark.sql.functions import round
df_employees_with_attendance = df_employees.join(df_attendance, on="EmpID", how="left")
df_employees_with_attendance = df_employees_with_attendance.groupBy("EmpID").agg(F.count(F.when(df_attendance.Status == "Present", 1)).alias("PresentDays"),F.count(F.when(df_attendance.Status.isNotNull(), 1)).alias("TotalDays"))
df_employees_with_attendance = df_employees_with_attendance.withColumn("AttendancePercentage", round((F.col("PresentDays") / F.col("TotalDays")) * 100, 2))
df_employees_with_attendance.show()

#Join employees and bonuses → Show top 3 employees by TotalCompensation.
df_bonuses_renamed = df_bonuses.withColumnRenamed("Bonus", "Bonus_renamed")
df_employees_with_bonuses = df_employees.join(df_bonuses_renamed, on="EmpID", how="left")
df_employees_with_bonuses = df_employees_with_bonuses.withColumn("TotalCompensation", df_employees_with_bonuses.Salary + df_employees_with_bonuses["Bonus_renamed"])
df_employees_with_bonuses.orderBy(F.desc("TotalCompensation")).limit(3).show()
#Multi-level join: employees + bonuses + attendance.
df_employees_with_bonuses_and_attendance = df_employees_with_bonuses.join(df_employees_with_attendance, on="EmpID", how="left")
df_employees_with_bonuses_and_attendance.show()

#Extract year and month from JoinDate.
from pyspark.sql.functions import year,month,col
df_employees = df_employees.withColumn("JoinYear", year(df_employees.JoinDate)).withColumn("JoinMonth", month(df_employees.JoinDate))
df_employees.show()
#Mask employee names using regex.
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
import re

def mask_name(name):
    name_parts = name.split(' ')
    if len(name_parts) == 2:
        first_name, last_name = name_parts
        masked_first_name = first_name[0] + '*' * (len(first_name) - 1)
        masked_last_name = last_name[0] + '*' * (len(last_name) - 1)
        return f"{masked_first_name} {masked_last_name}"
    elif len(name_parts) == 1:
        return name[0] + '*' * (len(name) - 1)
    else:
        return name

mask_name_udf = udf(mask_name, StringType())
customers_df = df_employees.withColumn("Name", mask_name_udf(col("Name")))
customers_df.show()

#Use substring() to create EmpCode like "EMP001".
from pyspark.sql.functions import col, lpad,concat,lit
df_employees_with_code = df_employees.withColumn("EmpCode",lpad("EmpID", 3, "0"))
df_employees_with_code = df_employees_with_code.withColumn("EmpCode",concat(lit("EMP"), col("EmpCode")))
df_employees_with_code.show()

#Use when/otherwise to label performance:
#“High” if Bonus > 6000
#“Medium” if 4000–6000
#“Low” otherwise
from pyspark.sql.functions import when
df_employees_with_performance = df_employees.withColumn("Performance", when(df_employees.Bonus > 6000, "High").when((df_employees.Bonus >= 4000) & (df_employees.Bonus <= 6000), "Medium").otherwise("Low"))
df_employees_with_performance.show()
#Handle missing ManagerID using fillna("No Manager").
df_employees = df_employees.fillna({"ManagerID": -1})
df_employees.show()
df_employees = df_employees.fillna({"ManagerID": "No Manager"})
df_employees.show()

# create and use database hr.
spark.sql("create database if not exists hr")
spark.sql("use hr")
# save all dataframes as tables: employees, attendance, bonuses.
df_employees.write.mode('overwrite').saveAsTable("employees")
df_attendance.write.mode('overwrite').saveAsTable("attendance")
df_bonuses.write.mode('overwrite').saveAsTable("bonuses")
# write sql queries:
# top paid employee in each department.
spark.sql("""select Department, max(Salary) as max_salary from employees group by Department""").show()
# attendance rate by department.
spark.sql("""select Department, count(case when status = 'Present' then 1 end) / count(*) as attendance_rate from employees e join attendance a on e.EmpID = a.EmpID group by Department""").show()
# employees joined after 2021 with salary > 70,000.
spark.sql("""select * from employees where JoinDate > '2021-01-01' and Salary > 70000""").show()

#Use a UDF to classify department as "Tech" vs "Non-Tech".
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

def classify_department(department):
    if department in ["Engineering"]:
        return "Tech"
    else:
        return "Non-Tech"

classify_department_udf = udf(classify_department, StringType())
df_employees = df_employees.withColumn("DepartmentType", classify_department_udf(df_employees.Department))
df_employees.show()
#Create a view emp_attendance_summary.
df_employees.createOrReplaceTempView("emp_attendance_summary")
#Save it as Parquet partitioned by Department.
df_employees.write.mode('overwrite').partitionBy("Department").parquet("emp_attendance_summary")