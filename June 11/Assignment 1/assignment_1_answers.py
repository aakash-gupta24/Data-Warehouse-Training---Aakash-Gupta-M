# -*- coding: utf-8 -*-
"""Assignment 1 answers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/177ev9nQjVLtWOtgSDuSho4tJdCsAKMlg
"""

from pyspark.sql import SparkSession

spark=SparkSession.builder.appName("azureToColab").getOrCreate()

spark

data = [
("Ananya", "HR", 52000),
("Rahul", "Engineering", 65000),
("Priya", "Engineering", 60000),
("Zoya", "Marketing", 48000),
("Karan", "HR", 53000),
("Naveen", "Engineering", 70000),
("Fatima", "Marketing", 45000)
]
columns = ["Name", "Department", "Salary"]
df = spark.createDataFrame(data, columns)
df.show()

#Exercise Set 1: Basics
#1. Display all records in the DataFrame.
df.show()
#2. Print the schema of the DataFrame.
df.printSchema()
#3. Count total number of employees.
df.select("Name").count()

#Exercise Set 2: Column Operations
#4. Add a new column Bonus which is 15% of Salary.
df=df.withColumn("Bonus", df["Salary"] * 0.15)
df.show()
#5. Add a new column NetPay = Salary + Bonus.
df=df.withColumn("NetPay", df["Salary"] + df["Bonus"])
df.show()

#Exercise Set 3: Filtering and Conditions
#6. Display only employees from the “Engineering” department.
df.filter(df["Department"] == "Engineering").show()
#7. Display employees whose salary is greater than 60000.
df.filter(df["Salary"] > 60000).show()
#8. Display employees who are not in the “Marketing” department.
df.filter(df["Department"] != "Marketing").show()

#Exercise Set 4: Sorting and Limiting
#9. Show top 3 highest paid employees.
df.orderBy(df["Salary"].desc()).limit(3).show()
#10. Sort the data by Department ascending and Salary descending.
df.orderBy(df["Department"].asc(), df["Salary"].desc()).show()

#Exercise Set 5: String and Case Logic
#11. Add a new column Level :
#“Senior” if salary > 60000
#“Mid” if salary between 50000 and 60000
#“Junior” otherwise
from pyspark.sql.functions import when
df=df.withColumn("Level", when(df["Salary"] > 60000, "Senior").when((df["Salary"] >= 50000) & (df["Salary"] <= 60000), "Mid").otherwise("Junior"))
df.show()
#  12. Convert all names to uppercase.
from pyspark.sql.functions import upper
df=df.withColumn("Name", upper(df["Name"]))
df.show()