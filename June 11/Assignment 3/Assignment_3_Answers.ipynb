{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "368XDp_wIDcK",
        "outputId": "0960ad8d-bc3e-4fbb-c85d-68317b1aab8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f43b8267450>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://10fd54f6dae1:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>June11Assignment3</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('June11Assignment3').getOrCreate()\n",
        "\n",
        "spark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "(\"Ananya\", \"HR\", 52000),\n",
        "(\"Rahul\", \"Engineering\", 65000),\n",
        "(\"Priya\", \"Engineering\", 60000),\n",
        "(\"Zoya\", \"Marketing\", 48000),\n",
        "(\"Karan\", \"HR\", 53000),\n",
        "(\"Naveen\", \"Engineering\", 70000),\n",
        "(\"Fatima\", \"Marketing\", 45000)\n",
        "]\n",
        "columns = [\"Name\", \"Department\", \"Salary\"]\n",
        "df_emp = spark.createDataFrame(data, columns)\n",
        "\n",
        "performance = [\n",
        "(\"Ananya\", 2023, 4.5),\n",
        "(\"Rahul\", 2023, 4.9),\n",
        "(\"Priya\", 2023, 4.3),\n",
        "(\"Zoya\", 2023, 3.8),\n",
        "(\"Karan\", 2023, 4.1),\n",
        "(\"Naveen\", 2023, 4.7),\n",
        "(\"Fatima\", 2023, 3.9)\n",
        "]\n",
        "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
        "df_perf = spark.createDataFrame(performance, columns_perf)\n",
        "\n",
        "project_data = [\n",
        "(\"Ananya\", \"HR Portal\", 120),\n",
        "(\"Rahul\", \"Data Platform\", 200),\n",
        "(\"Priya\", \"Data Platform\", 180),\n",
        "(\"Zoya\", \"Campaign Tracker\", 100),\n",
        "(\"Karan\", \"HR Portal\", 130),\n",
        "(\"Naveen\", \"ML Pipeline\", 220),\n",
        "(\"Fatima\", \"Campaign Tracker\", 90)\n",
        "]\n",
        "columns_proj = [\"Name\", \"Project\", \"HoursWorked\"]\n",
        "df_proj = spark.createDataFrame(project_data, columns_proj)"
      ],
      "metadata": {
        "id": "-1nHD8VcINvX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Joins and Advanced Aggregations\n",
        "#1. Join employee_data ,  performance_data , and project_data.\n",
        "df_joined = df_emp.join(df_perf, on=\"Name\", how=\"outer\")\n",
        "df_final = df_joined.join(df_proj, on=\"Name\", how=\"outer\")\n",
        "df_final.show()\n",
        "#2. Compute total hours worked per department.\n",
        "from pyspark.sql.functions import sum\n",
        "df_hours_worked = df_final.groupBy(\"Department\").agg(sum(\"HoursWorked\").alias(\"TotalHoursWorked\"))\n",
        "df_hours_worked.show()\n",
        "#3. Compute average rating per project.\n",
        "from pyspark.sql.functions import avg\n",
        "df_avg_rating = df_final.groupBy(\"Project\").agg(avg(\"Rating\").alias(\"AverageRating\"))\n",
        "df_avg_rating.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiUDWYE2IOMQ",
        "outputId": "9492c22c-994a-4f4c-8d79-e97cfd2c70bc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+----+------+----------------+-----------+\n",
            "|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|\n",
            "+------+-----------+------+----+------+----------------+-----------+\n",
            "|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|\n",
            "|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|\n",
            "| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|\n",
            "|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|\n",
            "| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|\n",
            "| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|\n",
            "|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|\n",
            "+------+-----------+------+----+------+----------------+-----------+\n",
            "\n",
            "+-----------+----------------+\n",
            "| Department|TotalHoursWorked|\n",
            "+-----------+----------------+\n",
            "|Engineering|             600|\n",
            "|         HR|             250|\n",
            "|  Marketing|             190|\n",
            "+-----------+----------------+\n",
            "\n",
            "+----------------+------------------+\n",
            "|         Project|     AverageRating|\n",
            "+----------------+------------------+\n",
            "|   Data Platform|               4.6|\n",
            "|       HR Portal|               4.3|\n",
            "|     ML Pipeline|               4.7|\n",
            "|Campaign Tracker|3.8499999999999996|\n",
            "+----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Handling Missing Data (introduce some manually)\n",
        "#4. Add a row to performance_data with a None rating.\n",
        "from pyspark.sql.functions import lit,when\n",
        "df_perf_with_none = df_perf.withColumn(\"Rating\", lit(None))\n",
        "df_perf_with_none.show()\n",
        "#5. Filter rows with null values.\n",
        "df_null_filtered = df_perf_with_none.filter(df_perf_with_none[\"Rating\"].isNull())\n",
        "df_null_filtered.show()\n",
        "#6. Replace null ratings with the department average.\n",
        "from pyspark.sql.window import Window\n",
        "df_replaced_null= df_final.withColumn(\"Rating\", when(df_final[\"Rating\"].isNull(), avg(df_final[\"Rating\"]).over(Window.partitionBy(\"Department\"))).otherwise(df_final[\"Rating\"]))\n",
        "df_replaced_null.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ9B7zngJk_D",
        "outputId": "eba5242d-9236-42b6-f05f-68942e392b15"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+------+\n",
            "|  Name|Year|Rating|\n",
            "+------+----+------+\n",
            "|Ananya|2023|  NULL|\n",
            "| Rahul|2023|  NULL|\n",
            "| Priya|2023|  NULL|\n",
            "|  Zoya|2023|  NULL|\n",
            "| Karan|2023|  NULL|\n",
            "|Naveen|2023|  NULL|\n",
            "|Fatima|2023|  NULL|\n",
            "+------+----+------+\n",
            "\n",
            "+------+----+------+\n",
            "|  Name|Year|Rating|\n",
            "+------+----+------+\n",
            "|Ananya|2023|  NULL|\n",
            "| Rahul|2023|  NULL|\n",
            "| Priya|2023|  NULL|\n",
            "|  Zoya|2023|  NULL|\n",
            "| Karan|2023|  NULL|\n",
            "|Naveen|2023|  NULL|\n",
            "|Fatima|2023|  NULL|\n",
            "+------+----+------+\n",
            "\n",
            "+------+-----------+------+----+------+----------------+-----------+\n",
            "|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|\n",
            "+------+-----------+------+----+------+----------------+-----------+\n",
            "|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|\n",
            "| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|\n",
            "| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|\n",
            "|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|\n",
            "| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|\n",
            "|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|\n",
            "|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|\n",
            "+------+-----------+------+----+------+----------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Built-In Functions and UDF\n",
        "#7. Create a column PerformanceCategory :\n",
        "# Excellent (>=4.7),\n",
        "# Good (4.0–4.69),\n",
        "# Average (<4.0)\n",
        "from pyspark.sql.functions import when\n",
        "df_performance_category = df_final.withColumn(\"PerformanceCategory\", when(df_final[\"Rating\"] >= 4.7, \"Excellent\").otherwise(when(df_final[\"Rating\"] >= 4.0, \"Good\").otherwise(\"Average\")))\n",
        "df_performance_category.show()\n",
        "#8. Create a UDF to assign bonus:\n",
        "#If project hours > 200 → 10,000\n",
        "#Else → 5,000\n",
        "def assign_bonus(hours_worked):\n",
        "    if hours_worked > 200:\n",
        "        return 10000\n",
        "    else:\n",
        "        return 5000\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "udf_assign_bonus = udf(assign_bonus, IntegerType())\n",
        "df_with_bonus = df_final.withColumn(\"Bonus\", udf_assign_bonus(df_final[\"HoursWorked\"]))\n",
        "df_with_bonus.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxWlCXvgLH6e",
        "outputId": "1182f288-549d-4cb9-9663-0c1637e8970a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+----+------+----------------+-----------+-------------------+\n",
            "|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|PerformanceCategory|\n",
            "+------+-----------+------+----+------+----------------+-----------+-------------------+\n",
            "|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|               Good|\n",
            "|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|            Average|\n",
            "| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|               Good|\n",
            "|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|          Excellent|\n",
            "| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|               Good|\n",
            "| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|          Excellent|\n",
            "|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|            Average|\n",
            "+------+-----------+------+----+------+----------------+-----------+-------------------+\n",
            "\n",
            "+------+-----------+------+----+------+----------------+-----------+-----+\n",
            "|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|Bonus|\n",
            "+------+-----------+------+----+------+----------------+-----------+-----+\n",
            "|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120| 5000|\n",
            "|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90| 5000|\n",
            "| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130| 5000|\n",
            "|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|10000|\n",
            "| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180| 5000|\n",
            "| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200| 5000|\n",
            "|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100| 5000|\n",
            "+------+-----------+------+----+------+----------------+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Date and Time Functions\n",
        "#9. Add a column JoinDate with 2021-06-01 for all, then add MonthsWorked as difference from today.\n",
        "from pyspark.sql.functions import lit, datediff, current_date\n",
        "df_with_join_date = df_final.withColumn(\"JoinDate\", lit(\"2021-06-01\"))\n",
        "df_with_months_worked = df_with_join_date.withColumn(\"MonthsWorked\", datediff(current_date(), \"JoinDate\") / 30)\n",
        "df_with_months_worked.show()\n",
        "# 10. Calculate how many employees joined before 2022.\n",
        "from pyspark.sql.functions import col\n",
        "df_before_2022 = df_final.filter(col(\"Year\") < 2022)\n",
        "count_before_2022 = df_before_2022.count()\n",
        "print(\"Number of employees joined before 2022:\", count_before_2022)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLPjlg9HMVjd",
        "outputId": "bec50e1f-b886-4ec1-e0ec-8113e29fe715"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+----+------+----------------+-----------+----------+-----------------+\n",
            "|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|  JoinDate|     MonthsWorked|\n",
            "+------+-----------+------+----+------+----------------+-----------+----------+-----------------+\n",
            "|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|2021-06-01|49.03333333333333|\n",
            "|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|2021-06-01|49.03333333333333|\n",
            "| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|2021-06-01|49.03333333333333|\n",
            "|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|2021-06-01|49.03333333333333|\n",
            "| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|2021-06-01|49.03333333333333|\n",
            "| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|2021-06-01|49.03333333333333|\n",
            "|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|2021-06-01|49.03333333333333|\n",
            "+------+-----------+------+----+------+----------------+-----------+----------+-----------------+\n",
            "\n",
            "Number of employees joined before 2022: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Unions\n",
        "#11. Create another small team DataFrame and union() it with employee_data.\n",
        "#extra_employees = [\n",
        "#(\"Meena\", \"HR\", 48000),\n",
        "#(\"Raj\", \"Marketing\", 51000)\n",
        "#]\n",
        "extra_employees = [\n",
        "(\"Meena\", \"HR\", 48000),\n",
        "(\"Raj\", \"Marketing\", 51000)\n",
        "]\n",
        "columns_extra = [\"Name\", \"Department\", \"Salary\"]\n",
        "df_extra_employees = spark.createDataFrame(extra_employees, columns_extra)\n",
        "df_unioned = df_emp.union(df_extra_employees)\n",
        "df_unioned.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KDoSquiNLeB",
        "outputId": "0ebd8e33-d31f-43d6-cbe0-6c7232abd465"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+\n",
            "|  Name| Department|Salary|\n",
            "+------+-----------+------+\n",
            "|Ananya|         HR| 52000|\n",
            "| Rahul|Engineering| 65000|\n",
            "| Priya|Engineering| 60000|\n",
            "|  Zoya|  Marketing| 48000|\n",
            "| Karan|         HR| 53000|\n",
            "|Naveen|Engineering| 70000|\n",
            "|Fatima|  Marketing| 45000|\n",
            "| Meena|         HR| 48000|\n",
            "|   Raj|  Marketing| 51000|\n",
            "+------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving Results\n",
        "#12.Save the final merged dataset (all 3 joins) as a partitioned Parquet file based on Department .\n",
        "df_final.write.partitionBy(\"Department\").parquet(\"df_final_partitioned.parquet\")\n"
      ],
      "metadata": {
        "id": "xOLc6RSuN48F"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "qbo1NAzAOCIt"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}