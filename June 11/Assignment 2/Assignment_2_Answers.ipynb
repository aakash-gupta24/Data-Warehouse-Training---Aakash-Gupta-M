{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "dsGHQAy0DYaj",
        "outputId": "4baf4eaa-e00e-4813-af39-968ca7fd27ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7bce33167610>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b1bee72e4040:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>June11Assignment2</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('June11Assignment2').getOrCreate()\n",
        "\n",
        "spark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "(\"Ananya\", \"HR\", 52000),\n",
        "(\"Rahul\", \"Engineering\", 65000),\n",
        "(\"Priya\", \"Engineering\", 60000),\n",
        "(\"Zoya\", \"Marketing\", 48000),\n",
        "(\"Karan\", \"HR\", 53000),\n",
        "(\"Naveen\", \"Engineering\", 70000),\n",
        "(\"Fatima\", \"Marketing\", 45000)\n",
        "]\n",
        "columns = [\"Name\", \"Department\", \"Salary\"]\n",
        "df_emp = spark.createDataFrame(data, columns)\n",
        "\n",
        "performance = [\n",
        "(\"Ananya\", 2023, 4.5),\n",
        "(\"Rahul\", 2023, 4.9),\n",
        "(\"Priya\", 2023, 4.3),\n",
        "(\"Zoya\", 2023, 3.8),\n",
        "(\"Karan\", 2023, 4.1),\n",
        "(\"Naveen\", 2023, 4.7),\n",
        "(\"Fatima\", 2023, 3.9)\n",
        "]\n",
        "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
        "df_perf = spark.createDataFrame(performance, columns_perf)"
      ],
      "metadata": {
        "id": "j7G4zdUyDx3q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GroupBy and Aggregations\n",
        "#1. Get the average salary by department.\n",
        "df_emp.groupBy(\"Department\").agg({\"Salary\": \"avg\"}).show()\n",
        "#2. Count of employees per department.\n",
        "df_emp.groupBy(\"Department\").agg({\"Name\": \"count\"}).show()\n",
        "#3. Maximum and minimum salary in Engineering.\n",
        "df_emp.filter(df_emp[\"Department\"] == \"Engineering\").agg({\"Salary\": \"max\"}).show()\n",
        "df_emp.filter(df_emp[\"Department\"] == \"Engineering\").agg({\"Salary\": \"min\"}).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0N65_2UEJ8G",
        "outputId": "3563f222-3793-43b5-8240-be58ef9ba2e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "| Department|avg(Salary)|\n",
            "+-----------+-----------+\n",
            "|Engineering|    65000.0|\n",
            "|         HR|    52500.0|\n",
            "|  Marketing|    46500.0|\n",
            "+-----------+-----------+\n",
            "\n",
            "+-----------+-----------+\n",
            "| Department|count(Name)|\n",
            "+-----------+-----------+\n",
            "|Engineering|          3|\n",
            "|         HR|          2|\n",
            "|  Marketing|          2|\n",
            "+-----------+-----------+\n",
            "\n",
            "+-----------+\n",
            "|max(Salary)|\n",
            "+-----------+\n",
            "|      70000|\n",
            "+-----------+\n",
            "\n",
            "+-----------+\n",
            "|min(Salary)|\n",
            "+-----------+\n",
            "|      60000|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join and Combine Data\n",
        "#4. Perform an inner join between employee_data and performance_data on Name .\n",
        "df_emp.join(df_perf, on=\"Name\",how='inner').show()\n",
        "#5. Show each employeeâ€™s salary and performance rating.\n",
        "df_emp.join(df_perf, on=\"Name\",how='inner').select(\"Name\",\"Salary\",\"Rating\").show()\n",
        "#6. Filter employees with rating > 4.5 and salary > 60000.\n",
        "df_emp.join(df_perf, on=\"Name\",how='inner').filter((df_perf[\"Rating\"] > 4.5) & (df_emp[\"Salary\"] > 60000)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTQaAHuWEcux",
        "outputId": "762204dc-bdef-4177-afa7-0e7e591f9c6a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+----+------+\n",
            "|  Name| Department|Salary|Year|Rating|\n",
            "+------+-----------+------+----+------+\n",
            "|Ananya|         HR| 52000|2023|   4.5|\n",
            "|Fatima|  Marketing| 45000|2023|   3.9|\n",
            "| Karan|         HR| 53000|2023|   4.1|\n",
            "|Naveen|Engineering| 70000|2023|   4.7|\n",
            "| Priya|Engineering| 60000|2023|   4.3|\n",
            "| Rahul|Engineering| 65000|2023|   4.9|\n",
            "|  Zoya|  Marketing| 48000|2023|   3.8|\n",
            "+------+-----------+------+----+------+\n",
            "\n",
            "+------+------+------+\n",
            "|  Name|Salary|Rating|\n",
            "+------+------+------+\n",
            "|Ananya| 52000|   4.5|\n",
            "|Fatima| 45000|   3.9|\n",
            "| Karan| 53000|   4.1|\n",
            "|Naveen| 70000|   4.7|\n",
            "| Priya| 60000|   4.3|\n",
            "| Rahul| 65000|   4.9|\n",
            "|  Zoya| 48000|   3.8|\n",
            "+------+------+------+\n",
            "\n",
            "+------+-----------+------+----+------+\n",
            "|  Name| Department|Salary|Year|Rating|\n",
            "+------+-----------+------+----+------+\n",
            "|Naveen|Engineering| 70000|2023|   4.7|\n",
            "| Rahul|Engineering| 65000|2023|   4.9|\n",
            "+------+-----------+------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Window & Rank (Bonus Challenge)\n",
        "#7. Rank employees by salary department-wise.\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank, col\n",
        "windowSpec = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
        "df_emp.withColumn(\"rank\",rank().over(windowSpec)).show()\n",
        "#8. Calculate cumulative salary in each department.\n",
        "from pyspark.sql.functions import sum\n",
        "df_emp.withColumn(\"cumulative_salary\",sum(col(\"Salary\")).over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqFQFPjFE3ve",
        "outputId": "be83e4f2-7457-4536-a85b-7c1c0027d8fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+----+\n",
            "|  Name| Department|Salary|rank|\n",
            "+------+-----------+------+----+\n",
            "|Naveen|Engineering| 70000|   1|\n",
            "| Rahul|Engineering| 65000|   2|\n",
            "| Priya|Engineering| 60000|   3|\n",
            "| Karan|         HR| 53000|   1|\n",
            "|Ananya|         HR| 52000|   2|\n",
            "|  Zoya|  Marketing| 48000|   1|\n",
            "|Fatima|  Marketing| 45000|   2|\n",
            "+------+-----------+------+----+\n",
            "\n",
            "+------+-----------+------+-----------------+\n",
            "|  Name| Department|Salary|cumulative_salary|\n",
            "+------+-----------+------+-----------------+\n",
            "|Naveen|Engineering| 70000|            70000|\n",
            "| Rahul|Engineering| 65000|           135000|\n",
            "| Priya|Engineering| 60000|           195000|\n",
            "| Karan|         HR| 53000|            53000|\n",
            "|Ananya|         HR| 52000|           105000|\n",
            "|  Zoya|  Marketing| 48000|            48000|\n",
            "|Fatima|  Marketing| 45000|            93000|\n",
            "+------+-----------+------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Date Operations\n",
        "#9. Add a new column JoinDate with random dates between 2020 and 2023.\n",
        "from datetime import datetime, timedelta\n",
        "from pyspark.sql import functions as F\n",
        "import random\n",
        "start_date = datetime(2020, 1, 1)\n",
        "end_date = datetime(2023, 12, 31)\n",
        "total_days = (end_date - start_date).days\n",
        "random_dates = [start_date + timedelta(days=random.randint(0, (end_date - start_date).days)) for _ in range(df_emp.count())]\n",
        "df_emp = df_emp.withColumn(\"JoinDate\", F.date_add(F.lit(start_date), (F.rand() * total_days).cast(\"int\")))\n",
        "df_emp.show()\n",
        "#10. Add column YearsWithCompany using current_date() and datediff() .\n",
        "from pyspark.sql.functions import current_date, datediff\n",
        "df_emp = df_emp.withColumn(\"YearsWithCompany\", datediff(current_date(), \"JoinDate\") / 365)\n",
        "df_emp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKIvZYTXGgoh",
        "outputId": "a973d1a4-d05d-44f9-93ce-e2887cd72da9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+----------+\n",
            "|  Name| Department|Salary|  JoinDate|\n",
            "+------+-----------+------+----------+\n",
            "|Ananya|         HR| 52000|2021-03-29|\n",
            "| Rahul|Engineering| 65000|2021-01-16|\n",
            "| Priya|Engineering| 60000|2023-05-18|\n",
            "|  Zoya|  Marketing| 48000|2021-08-11|\n",
            "| Karan|         HR| 53000|2020-04-30|\n",
            "|Naveen|Engineering| 70000|2021-05-13|\n",
            "|Fatima|  Marketing| 45000|2020-07-07|\n",
            "+------+-----------+------+----------+\n",
            "\n",
            "+------+-----------+------+----------+------------------+\n",
            "|  Name| Department|Salary|  JoinDate|  YearsWithCompany|\n",
            "+------+-----------+------+----------+------------------+\n",
            "|Ananya|         HR| 52000|2021-03-29| 4.205479452054795|\n",
            "| Rahul|Engineering| 65000|2021-01-16| 4.402739726027397|\n",
            "| Priya|Engineering| 60000|2023-05-18|2.0684931506849313|\n",
            "|  Zoya|  Marketing| 48000|2021-08-11| 3.835616438356164|\n",
            "| Karan|         HR| 53000|2020-04-30| 5.117808219178082|\n",
            "|Naveen|Engineering| 70000|2021-05-13| 4.082191780821918|\n",
            "|Fatima|  Marketing| 45000|2020-07-07| 4.931506849315069|\n",
            "+------+-----------+------+----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Writing to Files\n",
        "#11. Write the full employee DataFrame to CSV with headers.\n",
        "df_emp.write.csv(\"employee_data.csv\", header=True)\n",
        "#12. Save the joined DataFrame to a Parquet file.\n",
        "df_emp.join(df_perf, on=\"Name\",how='inner').write.parquet(\"joined_data.parquet\")"
      ],
      "metadata": {
        "id": "Dx6ZWnY6HWCq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "4BCL8NcKHfIa"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}