{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a2f92d-464d-4fd8-9fd7-f34c64db159c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=833800893595081#setting/sparkui/0611-043338-tjd8m6e5/driver-717835484911746958\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7c2b251e1450>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"June12Assignment1\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76406677-0b6a-43b4-8841-2f3ffc892368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------------------------------------------------+------+------+\n|OrderID|Customer|Items                                                         |Region|Amount|\n+-------+--------+--------------------------------------------------------------+------+------+\n|101    |Ali     |[{Product -> Laptop, Qty -> 1}, {Product -> Mouse, Qty -> 2}] |Asia  |1200.0|\n|102    |Zara    |[{Product -> Tablet, Qty -> 1}]                               |Europe|650.0 |\n|103    |Mohan   |[{Product -> Phone, Qty -> 2}, {Product -> Charger, Qty -> 1}]|Asia  |890.0 |\n|104    |Sara    |[{Product -> Desk, Qty -> 1}]                                 |US    |450.0 |\n+-------+--------+--------------------------------------------------------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row \n",
    "data = [ \n",
    "Row(OrderID=101, Customer=\"Ali\", Items=[{\"Product\":\"Laptop\", \"Qty\":1}, \n",
    "{\"Product\":\"Mouse\", \"Qty\":2}], Region=\"Asia\", Amount=1200.0), \n",
    "Row(OrderID=102, Customer=\"Zara\", Items=[{\"Product\":\"Tablet\", \"Qty\":1}], \n",
    "Region=\"Europe\", Amount=650.0), \n",
    "Row(OrderID=103, Customer=\"Mohan\", Items=[{\"Product\":\"Phone\", \"Qty\":2}, \n",
    "{\"Product\":\"Charger\", \"Qty\":1}], Region=\"Asia\", Amount=890.0), \n",
    "Row(OrderID=104, Customer=\"Sara\", Items=[{\"Product\":\"Desk\", \"Qty\":1}], \n",
    "Region=\"US\", Amount=450.0) \n",
    "] \n",
    "df_sales = spark.createDataFrame(data) \n",
    "df_sales.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e24ece-6452-4a6b-ac6f-33b6e278739c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------------------+------+------+\n|OrderID|Customer|Items                         |Region|Amount|\n+-------+--------+------------------------------+------+------+\n|101    |Ali     |{Product -> Laptop, Qty -> 1} |Asia  |1200.0|\n|101    |Ali     |{Product -> Mouse, Qty -> 2}  |Asia  |1200.0|\n|102    |Zara    |{Product -> Tablet, Qty -> 1} |Europe|650.0 |\n|103    |Mohan   |{Product -> Phone, Qty -> 2}  |Asia  |890.0 |\n|103    |Mohan   |{Product -> Charger, Qty -> 1}|Asia  |890.0 |\n|104    |Sara    |{Product -> Desk, Qty -> 1}   |US    |450.0 |\n+-------+--------+------------------------------+------+------+\n\n+-------+-------------+\n|Product|TotalQuantity|\n+-------+-------------+\n|Laptop |1.0          |\n|Mouse  |2.0          |\n|Tablet |1.0          |\n|Phone  |2.0          |\n|Charger|1.0          |\n|Desk   |1.0          |\n+-------+-------------+\n\n+------+-----------+\n|Region|TotalOrders|\n+------+-----------+\n|Europe|1          |\n|US    |1          |\n|Asia  |2          |\n+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Working with JSON & Nested Fields\n",
    "# 1. Flatten the Items array using explode() to create one row per product.\n",
    "from pyspark.sql.functions import explode, col, sum, count, countDistinct\n",
    "df_sales = df_sales.withColumn(\"Items\", explode(col(\"Items\")))\n",
    "df_sales.show(truncate=False)\n",
    "# 2. Count total quantity sold per product.\n",
    "df_sales.groupBy(\"Items.Product\").agg(sum(\"Items.Qty\").alias(\"TotalQuantity\")).show(truncate=False)\n",
    "# 3. Count number of orders per region.\n",
    "df_sales.groupBy(\"Region\").agg(countDistinct(\"OrderID\").alias(\"TotalOrders\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b7f8d66-0c86-4d3b-9bc5-8f283aa6c892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------------------+------+------+--------------+\n|OrderID|Customer|Items                         |Region|Amount|HighValueOrder|\n+-------+--------+------------------------------+------+------+--------------+\n|101    |Ali     |{Product -> Laptop, Qty -> 1} |Asia  |1200.0|Yes           |\n|101    |Ali     |{Product -> Mouse, Qty -> 2}  |Asia  |1200.0|Yes           |\n|102    |Zara    |{Product -> Tablet, Qty -> 1} |Europe|650.0 |No            |\n|103    |Mohan   |{Product -> Phone, Qty -> 2}  |Asia  |890.0 |No            |\n|103    |Mohan   |{Product -> Charger, Qty -> 1}|Asia  |890.0 |No            |\n|104    |Sara    |{Product -> Desk, Qty -> 1}   |US    |450.0 |No            |\n+-------+--------+------------------------------+------+------+--------------+\n\n+-------+--------+------------------------------+------+------+--------------+------------+\n|OrderID|Customer|Items                         |Region|Amount|HighValueOrder|ShippingZone|\n+-------+--------+------------------------------+------+------+--------------+------------+\n|101    |Ali     |{Product -> Laptop, Qty -> 1} |Asia  |1200.0|Yes           |Zone A      |\n|101    |Ali     |{Product -> Mouse, Qty -> 2}  |Asia  |1200.0|Yes           |Zone A      |\n|102    |Zara    |{Product -> Tablet, Qty -> 1} |Europe|650.0 |No            |Zone B      |\n|103    |Mohan   |{Product -> Phone, Qty -> 2}  |Asia  |890.0 |No            |Zone A      |\n|103    |Mohan   |{Product -> Charger, Qty -> 1}|Asia  |890.0 |No            |Zone A      |\n|104    |Sara    |{Product -> Desk, Qty -> 1}   |US    |450.0 |No            |Zone C      |\n+-------+--------+------------------------------+------+------+--------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using when and otherwise\n",
    "# 4. Create a new column HighValueOrder :\n",
    "# \"Yes\" if Amount > 1000\n",
    "# \"No\" otherwise\n",
    "from pyspark.sql.functions import when\n",
    "df_sales = df_sales.withColumn(\"HighValueOrder\", when(col(\"Amount\") > 1000, \"Yes\").otherwise(\"No\"))\n",
    "df_sales.show(truncate=False)\n",
    "# 5. Add a column ShippingZone :\n",
    "# Asia → \"Zone A\", Europe → \"Zone B\", US → \"Zone C\"\n",
    "df_sales = df_sales.withColumn(\"ShippingZone\", when(col(\"Region\") == \"Asia\", \"Zone A\").when(col(\"Region\") == \"Europe\", \"Zone B\").when(col(\"Region\") == \"US\", \"Zone C\").otherwise(\"Zone D\"))\n",
    "df_sales.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28d20779-e3d3-4d95-a2d6-e2926b0d9b53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---------+\n|Region|TotalOrders|AvgAmount|\n+------+-----------+---------+\n|Asia  |4          |1045.0   |\n|Europe|1          |650.0    |\n|US    |1          |450.0    |\n+------+-----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Temporary & Permanent Views\n",
    "# 6. Register df_sales as a temporary view named sales_view .\n",
    "df_sales.createOrReplaceTempView(\"sales_view\")\n",
    "# 7. Write a SQL query to:\n",
    "# Count orders by Region\n",
    "# Find average amount per region\n",
    "spark.sql(\"SELECT Region, COUNT(OrderID) AS TotalOrders, AVG(Amount) AS AvgAmount FROM sales_view GROUP BY Region\").show(truncate=False)\n",
    "# 8. Create a permanent view using saveAsTable() .\n",
    "df_sales.write.saveAsTable(\"sales_permanent_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8376c203-fd88-417c-994b-a5221bc7be6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n|Region|OrderCount|\n+------+----------+\n|  Asia|         4|\n|Europe|         1|\n|    US|         1|\n+------+----------+\n\n+--------+\n|Customer|\n+--------+\n|     Ali|\n|     Ali|\n|    Zara|\n|   Mohan|\n|   Mohan|\n|    Sara|\n+--------+\n\n+--------+\n|Customer|\n+--------+\n|     Ali|\n|     Ali|\n|   Mohan|\n|   Mohan|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# SQL Queries via Spark\n",
    "spark.sql(\"SELECT Region, COUNT(*) as OrderCount FROM sales_view GROUP BY Region\").show()\n",
    "\n",
    "# 9. Use SQL to filter all orders with more than 1 item.\n",
    "spark.sql(\"SELECT Customer FROM sales_view WHERE size(Items) > 1\").show()\n",
    "# 10. Use SQL to extract customer names where Amount > 800.\n",
    "spark.sql(\"SELECT Customer FROM sales_view WHERE Amount > 800\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece2292b-b39e-44e6-adad-2160892034f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n|Product|TotalQuantity|\n+-------+-------------+\n|Phone  |2.0          |\n|Charger|1.0          |\n|Laptop |1.0          |\n|Mouse  |2.0          |\n|Tablet |1.0          |\n|Desk   |1.0          |\n+-------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Saving as Parquet and Reading Again\n",
    "# 11. Save the exploded product-level DataFrame as a partitioned Parquet file by\n",
    "# Region .\n",
    "df_sales.write.mode(\"overwrite\").partitionBy(\"Region\").parquet(\"dbfs:/FileStore/shared_uploads/parquet/\")\n",
    "# 12. Read the parquet back and perform a group-by on Product .\n",
    "spark.read.parquet(\"dbfs:/FileStore/shared_uploads/parquet/\").groupBy(\"Items.Product\").agg(sum(\"Items.Qty\").alias(\"TotalQuantity\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67a82e61-506b-4854-a607-4e48da036d52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "June12Assignment1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}