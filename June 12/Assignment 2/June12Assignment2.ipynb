{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15154465-c191-4902-802e-a2696da3acfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=833800893595081#setting/sparkui/0611-043338-tjd8m6e5/driver-372553966848630217\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x768f6c249510>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"June12Assignment2\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "418905b3-3611-4aee-8b9b-4d720ee8bd64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------+--------+-------+-------+\n|UserID|Page    |Timestamp          |Duration|Device |Country|\n+------+--------+-------------------+--------+-------+-------+\n|1     |Home    |2024-04-10 10:00:00|35      |Mobile |India  |\n|2     |Products|2024-04-10 10:02:00|120     |Desktop|USA    |\n|3     |Cart    |2024-04-10 10:05:00|45      |Tablet |UK     |\n|1     |Checkout|2024-04-10 10:08:00|60      |Mobile |India  |\n|4     |Home    |2024-04-10 10:10:00|15      |Mobile |Canada |\n|2     |Contact |2024-04-10 10:15:00|25      |Desktop|USA    |\n|5     |Products|2024-04-10 10:20:00|90      |Desktop|India  |\n+------+--------+-------------------+--------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "web_data = [\n",
    "Row(UserID=1, Page=\"Home\", Timestamp=\"2024-04-10 10:00:00\", Duration=35,\n",
    "Device=\"Mobile\", Country=\"India\"),\n",
    "Row(UserID=2, Page=\"Products\", Timestamp=\"2024-04-10 10:02:00\", Duration=120,\n",
    "Device=\"Desktop\", Country=\"USA\"),\n",
    "Row(UserID=3, Page=\"Cart\", Timestamp=\"2024-04-10 10:05:00\", Duration=45,\n",
    "Device=\"Tablet\", Country=\"UK\"),\n",
    "Row(UserID=1, Page=\"Checkout\", Timestamp=\"2024-04-10 10:08:00\", Duration=60,\n",
    "Device=\"Mobile\", Country=\"India\"),\n",
    "Row(UserID=4, Page=\"Home\", Timestamp=\"2024-04-10 10:10:00\", Duration=15,\n",
    "Device=\"Mobile\", Country=\"Canada\"),\n",
    "Row(UserID=2, Page=\"Contact\", Timestamp=\"2024-04-10 10:15:00\", Duration=25,\n",
    "Device=\"Desktop\", Country=\"USA\"),\n",
    "Row(UserID=5, Page=\"Products\", Timestamp=\"2024-04-10 10:20:00\", Duration=90,\n",
    "Device=\"Desktop\", Country=\"India\"),\n",
    "]\n",
    "df_web = spark.createDataFrame(web_data)\n",
    "df_web.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0278dc45-b27a-42ff-8b71-311b8d83e6e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- UserID: long (nullable = true)\n |-- Page: string (nullable = true)\n |-- Timestamp: string (nullable = true)\n |-- Duration: long (nullable = true)\n |-- Device: string (nullable = true)\n |-- Country: string (nullable = true)\n\n+------+--------+-------------------+--------+-------+-------+\n|UserID|Page    |Timestamp          |Duration|Device |Country|\n+------+--------+-------------------+--------+-------+-------+\n|1     |Home    |2024-04-10 10:00:00|35      |Mobile |India  |\n|2     |Products|2024-04-10 10:02:00|120     |Desktop|USA    |\n|3     |Cart    |2024-04-10 10:05:00|45      |Tablet |UK     |\n|1     |Checkout|2024-04-10 10:08:00|60      |Mobile |India  |\n|4     |Home    |2024-04-10 10:10:00|15      |Mobile |Canada |\n|2     |Contact |2024-04-10 10:15:00|25      |Desktop|USA    |\n|5     |Products|2024-04-10 10:20:00|90      |Desktop|India  |\n+------+--------+-------------------+--------+-------+-------+\n\n+------+--------+-------------------+--------+-------+-------+-------------+\n|UserID|Page    |Timestamp          |Duration|Device |Country|SessionMinute|\n+------+--------+-------------------+--------+-------+-------+-------------+\n|1     |Home    |2024-04-10 10:00:00|35      |Mobile |India  |0            |\n|2     |Products|2024-04-10 10:02:00|120     |Desktop|USA    |2            |\n|3     |Cart    |2024-04-10 10:05:00|45      |Tablet |UK     |5            |\n|1     |Checkout|2024-04-10 10:08:00|60      |Mobile |India  |8            |\n|4     |Home    |2024-04-10 10:10:00|15      |Mobile |Canada |10           |\n|2     |Contact |2024-04-10 10:15:00|25      |Desktop|USA    |15           |\n|5     |Products|2024-04-10 10:20:00|90      |Desktop|India  |20           |\n+------+--------+-------------------+--------+-------+-------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration & Preparation\n",
    "# 1. Display the schema of web_traffic_data .\n",
    "df_web.printSchema()\n",
    "# 2. Convert the Timestamp column to a proper timestamp type.\n",
    "df_web = df_web.withColumn(\"Timestamp\", df_web[\"Timestamp\"].cast(\"timestamp\"))\n",
    "df_web.show(truncate=False)\n",
    "# 3. Add a new column SessionMinute by extracting the minute from the Timestamp .\n",
    "from pyspark.sql import functions as F\n",
    "df_web = df_web.withColumn(\"SessionMinute\", F.minute(df_web[\"Timestamp\"]))\n",
    "df_web.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b724a578-b838-4bf1-9ffd-93d13f9bb7ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------+--------+------+-------+-------------+\n|UserID|Page    |Timestamp          |Duration|Device|Country|SessionMinute|\n+------+--------+-------------------+--------+------+-------+-------------+\n|1     |Checkout|2024-04-10 10:08:00|60      |Mobile|India  |8            |\n+------+--------+-------------------+--------+------+-------+-------------+\n\n+------+--------+-------------------+--------+-------+-------+-------------+\n|UserID|Page    |Timestamp          |Duration|Device |Country|SessionMinute|\n+------+--------+-------------------+--------+-------+-------+-------------+\n|2     |Products|2024-04-10 10:02:00|120     |Desktop|USA    |2            |\n|5     |Products|2024-04-10 10:20:00|90      |Desktop|India  |20           |\n+------+--------+-------------------+--------+-------+-------+-------------+\n\n+------+--------+-------------------+--------+-------+-------+-------------+\n|UserID|Page    |Timestamp          |Duration|Device |Country|SessionMinute|\n+------+--------+-------------------+--------+-------+-------+-------------+\n|5     |Products|2024-04-10 10:20:00|90      |Desktop|India  |20           |\n+------+--------+-------------------+--------+-------+-------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filtering and Conditions\n",
    "# 4. Filter users who used a \"Mobile\" device and visited the \"Checkout\" page.\n",
    "df_web.filter((df_web[\"Device\"] == \"Mobile\") & (df_web[\"Page\"] == \"Checkout\")).show(truncate=False)\n",
    "# 5. Show all entries with a Duration greater than 60 seconds.\n",
    "df_web.filter(df_web[\"Duration\"] > 60).show(truncate=False)\n",
    "# 6. Find all users from India who visited the \"Products\" page.\n",
    "df_web.filter((df_web[\"Country\"] == \"India\") & (df_web[\"Page\"] == \"Products\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfbd520b-bb5c-40b5-bc11-6dcde43dcc8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n|Device |AvgDuration       |\n+-------+------------------+\n|Mobile |36.666666666666664|\n|Tablet |45.0              |\n|Desktop|78.33333333333333 |\n+-------+------------------+\n\n+--------+-----+\n|Page    |count|\n+--------+-----+\n|Home    |2    |\n|Products|2    |\n|Cart    |1    |\n|Checkout|1    |\n|Contact |1    |\n+--------+-----+\n\n+--------+-----+\n|Page    |count|\n+--------+-----+\n|Home    |2    |\n|Products|2    |\n|Cart    |1    |\n|Checkout|1    |\n|Contact |1    |\n+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Aggregation and Grouping\n",
    "# 7. Get the average duration per device type.\n",
    "df_web.groupBy(\"Device\").agg(F.avg(\"Duration\").alias(\"AvgDuration\")).show(truncate=False)\n",
    "# 8. Count the number of sessions per country.\n",
    "df_web.groupBy(\"Page\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "# 9. Find the most visited page overall.\n",
    "df_web.groupBy(\"Page\").count().orderBy(F.desc(\"count\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be4b63a6-5ebd-4f52-bc01-29b9a09bdf0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------+--------+-------+-------+-------------+----+\n|UserID|Page    |Timestamp          |Duration|Device |Country|SessionMinute|Rank|\n+------+--------+-------------------+--------+-------+-------+-------------+----+\n|1     |Home    |2024-04-10 10:00:00|35      |Mobile |India  |0            |1   |\n|1     |Checkout|2024-04-10 10:08:00|60      |Mobile |India  |8            |2   |\n|2     |Products|2024-04-10 10:02:00|120     |Desktop|USA    |2            |1   |\n|2     |Contact |2024-04-10 10:15:00|25      |Desktop|USA    |15           |2   |\n|3     |Cart    |2024-04-10 10:05:00|45      |Tablet |UK     |5            |1   |\n|4     |Home    |2024-04-10 10:10:00|15      |Mobile |Canada |10           |1   |\n|5     |Products|2024-04-10 10:20:00|90      |Desktop|India  |20           |1   |\n+------+--------+-------------------+--------+-------+-------+-------------+----+\n\n+------+-------------+\n|UserID|TotalDuration|\n+------+-------------+\n|1     |95           |\n|3     |45           |\n|2     |145          |\n|4     |15           |\n|5     |90           |\n+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Window Functions\n",
    "# 10. Rank each userâ€™s pages by timestamp (oldest to newest).\n",
    "from pyspark.sql.window import Window\n",
    "windowSpec = Window.partitionBy(\"UserID\").orderBy(F.asc(\"Timestamp\"))\n",
    "df_web = df_web.withColumn(\"Rank\", F.rank().over(windowSpec))\n",
    "df_web.show(truncate=False)\n",
    "# 11. Find the total duration of all sessions per user using groupBy .\n",
    "df_web.groupBy(\"UserID\").agg(F.sum(\"Duration\").alias(\"TotalDuration\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6fc4da-9a9f-42a1-b56b-1d7554e008f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n|Page    |Duration|\n+--------+--------+\n|Products|120     |\n|Products|90      |\n+--------+--------+\n\n+--------+-----------+\n|Page    |UniqueUsers|\n+--------+-----------+\n|Cart    |1          |\n|Home    |2          |\n|Checkout|1          |\n|Products|2          |\n|Contact |1          |\n+--------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL Tasks\n",
    "# 12. Create a temporary view called traffic_view .\n",
    "df_web.createOrReplaceTempView(\"traffic_view\")\n",
    "# 13. Write a SQL query to get the top 2 longest sessions by duration.\n",
    "spark.sql(\"select Page, Duration from traffic_view order by Duration desc limit 2\").show(truncate=False)\n",
    "# 14. Get the number of unique users per page using SQL.\n",
    "spark.sql(\"select Page, count(distinct UserID) as UniqueUsers from traffic_view group by Page\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caa6848c-b23e-4263-9910-fe3b9d95fbcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Export & Save\n",
    "# 15. Save the final DataFrame to CSV.\n",
    "df_web.write.mode(\"overwrite\").csv(\"dbfs:/FileStore/tables/traffic_data.csv\", header=True)\n",
    "# 16. Save partitioned by Country in Parquet format.\n",
    "df_web.write.mode(\"overwrite\").partitionBy(\"Country\").parquet(\"dbfs:/FileStore/tables/traffic_data_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2231349-c34b-4bfc-91c4-e51a2cbe50c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7610974577415558>, line 3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Export & Save\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# 15. Save the final DataFrame to CSV.\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m df_web\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/tables/traffic_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# 16. Save partitioned by Country in Parquet format.\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m df_web\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCountry\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/tables/traffic_data_partitioned\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:2133\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n",
       "\u001B[1;32m   2114\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n",
       "\u001B[1;32m   2115\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n",
       "\u001B[1;32m   2116\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m   2117\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2131\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n",
       "\u001B[1;32m   2132\u001B[0m )\n",
       "\u001B[0;32m-> 2133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39mcsv(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [PATH_ALREADY_EXISTS] Path dbfs:/FileStore/tables/traffic_data.csv already exists. Set mode as \"overwrite\" to overwrite the existing path. SQLSTATE: 42K04"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[PATH_ALREADY_EXISTS] Path dbfs:/FileStore/tables/traffic_data.csv already exists. Set mode as \"overwrite\" to overwrite the existing path. SQLSTATE: 42K04"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "PATH_ALREADY_EXISTS",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "42K04",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-7610974577415558>, line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Export & Save\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# 15. Save the final DataFrame to CSV.\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m df_web\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/tables/traffic_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# 16. Save partitioned by Country in Parquet format.\u001B[39;00m\n\u001B[1;32m      5\u001B[0m df_web\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCountry\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/tables/traffic_data_partitioned\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:2133\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   2114\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m   2115\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m   2116\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m   2117\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2131\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[1;32m   2132\u001B[0m )\n\u001B[0;32m-> 2133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39mcsv(path)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [PATH_ALREADY_EXISTS] Path dbfs:/FileStore/tables/traffic_data.csv already exists. Set mode as \"overwrite\" to overwrite the existing path. SQLSTATE: 42K04"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "June12Assignment2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}