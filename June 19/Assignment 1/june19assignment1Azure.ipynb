{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbcc9071-d512-484a-abaf-b8cb26529308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=833800893595081#setting/sparkui/0611-043338-tjd8m6e5/driver-1017925573377100288\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7acc35c19510>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"june19assignment1\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc0c3db8-5f46-4c03-aebf-1b4df2342d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LogID: string (nullable = true)\n",
      " |-- VehicleID: string (nullable = true)\n",
      " |-- EntryPoint: string (nullable = true)\n",
      " |-- ExitPoint: string (nullable = true)\n",
      " |-- EntryTime: timestamp (nullable = true)\n",
      " |-- ExitTime: timestamp (nullable = true)\n",
      " |-- VehicleType: string (nullable = true)\n",
      " |-- SpeedKMH: integer (nullable = true)\n",
      " |-- TollPaid: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- LogID: string (nullable = true)\n",
      " |-- VehicleID: string (nullable = true)\n",
      " |-- EntryPoint: string (nullable = true)\n",
      " |-- ExitPoint: string (nullable = true)\n",
      " |-- EntryTime: timestamp (nullable = true)\n",
      " |-- ExitTime: timestamp (nullable = true)\n",
      " |-- VehicleType: string (nullable = true)\n",
      " |-- SpeedKMH: integer (nullable = true)\n",
      " |-- TollPaid: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- LogID: string (nullable = true)\n",
      " |-- VehicleID: string (nullable = true)\n",
      " |-- EntryPoint: string (nullable = true)\n",
      " |-- ExitPoint: string (nullable = true)\n",
      " |-- EntryTime: timestamp (nullable = true)\n",
      " |-- ExitTime: timestamp (nullable = true)\n",
      " |-- VehicleType: string (nullable = true)\n",
      " |-- SpeedKMH: integer (nullable = true)\n",
      " |-- TollPaid: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Ingestion & Schema Analysis\n",
    "# Load CSV using PySpark with schema inference\n",
    "spark.conf.set(\"fs.azure.account.key.hestore.blob.core.windows.net\",\"---------AccessKeyyy----------\")\n",
    "\n",
    "traffic_log_df=spark.read.csv(\"wasbs://june19assignment1@hestore.blob.core.windows.net/traffic_logs.csv\",header=True,inferSchema=True)\n",
    "\n",
    "traffic_log_df.printSchema()\n",
    "# Manually define schema and compare\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "traffic_log_schema=StructType([StructField(\"LogID\",StringType(),True),StructField(\"VehicleID\",StringType(),True),StructField(\"EntryPoint\",StringType(),True),StructField(\"ExitPoint\",StringType(),True),StructField(\"EntryTime\",TimestampType(),True),StructField(\"ExitTime\",TimestampType(),True),StructField(\"VehicleType\",StringType(),True),StructField(\"SpeedKMH\",IntegerType(),True),StructField(\"TollPaid\",IntegerType(),True)])\n",
    "\n",
    "traffic_log_df=spark.read.csv(\"wasbs://june19assignment1@hestore.blob.core.windows.net/traffic_logs.csv\",header=True,schema=traffic_log_schema)\n",
    "traffic_log_df.printSchema()\n",
    "# Ensure EntryTime/ExitTime are timestamp\n",
    "traffic_log_df=spark.read.csv(\"wasbs://june19assignment1@hestore.blob.core.windows.net/traffic_logs.csv\",header=True,inferSchema=True)\n",
    "traffic_log_df=traffic_log_df.withColumn(\"EntryTime\",to_timestamp(\"EntryTime\"))\n",
    "traffic_log_df=traffic_log_df.withColumn(\"ExitTime\",to_timestamp(\"ExitTime\"))\n",
    "traffic_log_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e36955-98f1-4918-99da-56ac7d359646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LogID: string (nullable = true)\n",
      " |-- VehicleID: string (nullable = true)\n",
      " |-- EntryPoint: string (nullable = true)\n",
      " |-- ExitPoint: string (nullable = true)\n",
      " |-- EntryTime: timestamp (nullable = true)\n",
      " |-- ExitTime: timestamp (nullable = true)\n",
      " |-- VehicleType: string (nullable = true)\n",
      " |-- SpeedKMH: integer (nullable = true)\n",
      " |-- TollPaid: integer (nullable = true)\n",
      " |-- TripDurationMinutes: double (nullable = true)\n",
      " |-- IsOverspeed: integer (nullable = false)\n",
      "\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+\n",
      "|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+\n",
      "| L001|     V001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|               19.0|          0|\n",
      "| L002|     V002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|     100|               35.0|          0|\n",
      "| L003|     V003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|               18.0|          0|\n",
      "| L004|     V004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|               20.0|          1|\n",
      "| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|               35.0|          0|\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Derived Column Creation\n",
    "# Calculate TripDurationMinutes = ExitTime - EntryTime\n",
    "traffic_log_df=traffic_log_df.withColumn(\"TripDurationMinutes\",round((unix_timestamp(\"ExitTime\")-unix_timestamp(\"EntryTime\"))/60))\n",
    "# Add IsOverspeed = SpeedKMH > 60\n",
    "traffic_log_df=traffic_log_df.withColumn(\"IsOverspeed\",when(col(\"SpeedKMH\")>60,1).otherwise(0))\n",
    "traffic_log_df.printSchema()\n",
    "traffic_log_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b07b5d-42c6-43db-9059-c65a486e036a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|VehicleType|AvgSpeedKMH|\n",
      "+-----------+-----------+\n",
      "|       Bike|       55.0|\n",
      "|        Car|       70.0|\n",
      "|      Truck|       45.0|\n",
      "|        Bus|       40.0|\n",
      "+-----------+-----------+\n",
      "\n",
      "+---------+-----+\n",
      "|ExitPoint|count|\n",
      "+---------+-----+\n",
      "|    GateD|    2|\n",
      "+---------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vehicle Behavior Aggregations\n",
    "# Average speed per VehicleType\n",
    "traffic_log_df.groupBy(\"VehicleType\").agg(avg(\"SpeedKMH\").alias(\"AvgSpeedKMH\")).show()\n",
    "# Total toll collected per gate (EntryPoint)\n",
    "traffic_log_df.groupBy(\"ExitPoint\").agg(sum(\"TollPaid\").alias(\"TotalTollPaid\")).show\n",
    "# Most used ExitPoint\n",
    "traffic_log_df.groupBy(\"ExitPoint\").count().orderBy(desc(\"count\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f35a5d3-04f8-4249-a4e9-5283b6a46836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+----+\n",
      "|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|Rank|\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+----+\n",
      "| L003|     V003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|               18.0|          0|   1|\n",
      "| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|               35.0|          0|   1|\n",
      "| L004|     V004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|               20.0|          1|   1|\n",
      "| L001|     V001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|               19.0|          0|   2|\n",
      "| L002|     V002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|     100|               35.0|          0|   1|\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+----+\n",
      "\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+----+------------+\n",
      "|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|Rank|LastExitTime|\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+----+------------+\n",
      "| L001|     V001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|               19.0|          0|   2|        NULL|\n",
      "| L002|     V002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|     100|               35.0|          0|   1|        NULL|\n",
      "| L003|     V003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|               18.0|          0|   1|        NULL|\n",
      "| L004|     V004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|               20.0|          1|   1|        NULL|\n",
      "| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|               35.0|          0|   1|        NULL|\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window Functions\n",
    "# Rank vehicles by speed within VehicleType\n",
    "from pyspark.sql.window import Window\n",
    "traffic_log_df_window=traffic_log_df.withColumn(\"Rank\",dense_rank().over(Window.partitionBy(\"VehicleType\").orderBy(desc(\"SpeedKMH\"))))\n",
    "t=traffic_log_df_window\n",
    "traffic_log_df_window.show()\n",
    "# Find last exit time for each vehicle using lag()\n",
    "traffic_log_df_window=traffic_log_df_window.withColumn(\"LastExitTime\",lag(\"ExitTime\",1).over(Window.partitionBy(\"VehicleID\").orderBy(desc(\"ExitTime\"))))\n",
    "traffic_log_df_window.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b59b2bc-a3db-4007-acbb-b03c3bc5b5ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+\n",
      "|VehicleID|       SessionStart|         SessionEnd|\n",
      "+---------+-------------------+-------------------+\n",
      "|     V004|2024-05-01 09:15:00|2024-05-01 09:35:00|\n",
      "|     V005|2024-05-01 10:05:00|2024-05-01 10:40:00|\n",
      "|     V001|2024-05-01 08:01:00|2024-05-01 08:20:00|\n",
      "|     V003|2024-05-01 09:00:00|2024-05-01 09:18:00|\n",
      "|     V002|2024-05-01 08:10:00|2024-05-01 08:45:00|\n",
      "+---------+-------------------+-------------------+\n",
      "\n",
      "+---------+-------------------+-------------------+--------+\n",
      "|VehicleID|       SessionStart|         SessionEnd|IdleTime|\n",
      "+---------+-------------------+-------------------+--------+\n",
      "|     V004|2024-05-01 09:15:00|2024-05-01 09:35:00|    20.0|\n",
      "|     V005|2024-05-01 10:05:00|2024-05-01 10:40:00|    35.0|\n",
      "|     V001|2024-05-01 08:01:00|2024-05-01 08:20:00|    19.0|\n",
      "|     V003|2024-05-01 09:00:00|2024-05-01 09:18:00|    18.0|\n",
      "|     V002|2024-05-01 08:10:00|2024-05-01 08:45:00|    35.0|\n",
      "+---------+-------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Session Segmentation\n",
    "# Group by VehicleID to simulate route sessions\n",
    "traffic_log_df_window=traffic_log_df_window.groupBy(\"VehicleID\").agg(min(\"EntryTime\").alias(\"SessionStart\"),max(\"ExitTime\").alias(\"SessionEnd\"))\n",
    "traffic_log_df_window.show()\n",
    "# Find duration between subsequent trips (idle time)\n",
    "traffic_log_df_window.withColumn(\"IdleTime\",round((unix_timestamp(\"SessionEnd\")-unix_timestamp(\"SessionStart\"))/60)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1cdc1da-10e2-441b-aaba-ca3f8abd03d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+----+\n",
      "|LogID|VehicleID|EntryPoint|ExitPoint|EntryTime|ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|Rank|\n",
      "+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+----+\n",
      "+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+----+\n",
      "\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+--------+\n",
      "|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|TripTime|\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+--------+\n",
      "| L001|     V001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|               19.0|          0|    19.0|\n",
      "| L003|     V003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|               18.0|          0|    18.0|\n",
      "| L004|     V004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|               20.0|          1|    20.0|\n",
      "| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|               35.0|          0|    35.0|\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+--------+\n",
      "\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+--------+\n",
      "|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|TripTime|\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+--------+\n",
      "| L001|     V001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|               19.0|          0|    19.0|\n",
      "| L002|     V002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|     100|               35.0|          0|    35.0|\n",
      "| L003|     V003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|               18.0|          0|    18.0|\n",
      "| L004|     V004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|               20.0|          1|    20.0|\n",
      "| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|               35.0|          0|    35.0|\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Anomaly Detection\n",
    "# Identify vehicles with speed > 70 and TripDuration < 10 minutes\n",
    "t.filter((col(\"SpeedKMH\")>70) & (col(\"TripDurationMinutes\")<10)).show()\n",
    "# Vehicles that paid less toll for longer trips\n",
    "traffic_log_df=traffic_log_df.withColumn(\"TripTime\",round((unix_timestamp(\"ExitTime\")-unix_timestamp(\"EntryTime\"))/60))\n",
    "traffic_log_df.filter((col(\"TripTime\")<60) & (col(\"TollPaid\")<100)).show()\n",
    "# Suspicious backtracking (ExitPoint earlier than EntryPoint)\n",
    "traffic_log_df.filter((col(\"ExitTime\")>col(\"EntryTime\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df634d83-3061-4b3c-82fc-ae3c6c493660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|RegisteredCity|count|\n",
      "+--------------+-----+\n",
      "|     Bangalore|    1|\n",
      "|       Chennai|    1|\n",
      "|        Mumbai|    1|\n",
      "|          Pune|    1|\n",
      "|         Delhi|    1|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "vehicle_data = [\n",
    "    Row(VehicleID=\"V001\", OwnerName=\"Anil\", Model=\"Hyundai i20\", RegisteredCity=\"Delhi\"),\n",
    "    Row(VehicleID=\"V002\", OwnerName=\"Rakesh\", Model=\"Tata Truck\", RegisteredCity=\"Chennai\"),\n",
    "    Row(VehicleID=\"V003\", OwnerName=\"Sana\", Model=\"Yamaha R15\", RegisteredCity=\"Mumbai\"),\n",
    "    Row(VehicleID=\"V004\", OwnerName=\"Neha\", Model=\"Honda City\", RegisteredCity=\"Bangalore\"),\n",
    "    Row(VehicleID=\"V005\", OwnerName=\"Zoya\", Model=\"Volvo Bus\", RegisteredCity=\"Pune\"),\n",
    "]\n",
    "\n",
    "df_registry = spark.createDataFrame(vehicle_data)\n",
    "\n",
    "# Join and group trips by RegisteredCity\n",
    "df_joined = traffic_log_df.join(df_registry, on=\"VehicleID\", how=\"left\")\n",
    "df_joined.groupBy(\"RegisteredCity\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2e1ba23-84a5-4ef3-ad94-b1442f1c5147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+----------------------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n",
      "|version|timestamp          |userId          |userName                          |operation|operationParameters                                                                                                                                                                 |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |userMetadata|engineInfo                                |\n",
      "+-------+-------------------+----------------+----------------------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n",
      "|10     |2025-06-19 05:51:26|8835572673210010|azuser3544_mml.local@techademy.com|OPTIMIZE |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                      |NULL|{3519917694817942}|0611-043338-tjd8m6e5|8          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 6223, p25FileSize -> 3329, numDeletionVectorsRemoved -> 1, conflictDetectionTimeMs -> 14, minFileSize -> 3329, numAddedFiles -> 1, maxFileSize -> 3329, p75FileSize -> 3329, p50FileSize -> 3329, numAddedBytes -> 3329}                                                                                                                                                                                                                                                                                                                                                                                                                 |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|9      |2025-06-19 05:51:25|8835572673210010|azuser3544_mml.local@techademy.com|DELETE   |{predicate -> [\"(TripDurationMinutes#33373 > 60.0)\"]}                                                                                                                               |NULL|{3519917694817942}|0611-043338-tjd8m6e5|8          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 151, numDeletionVectorsUpdated -> 0, numDeletedRows -> 0, scanTimeMs -> 151, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0}                                                                                                                                                                                                                                                                                                                                                               |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|8      |2025-06-19 05:51:24|8835572673210010|azuser3544_mml.local@techademy.com|MERGE    |{predicate -> [\"(LogID#33364 = LogID#32116)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> []}|NULL|{3519917694817942}|0611-043338-tjd8m6e5|7          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 2901, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 1648, materializeSourceTimeMs -> 160, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 618, numTargetRowsUpdated -> 1, numOutputRows -> 1, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 1, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 847}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|7      |2025-06-19 05:51:21|8835572673210010|azuser3544_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                        |NULL|{3519917694817942}|0611-043338-tjd8m6e5|6          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 3322}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|6      |2025-06-19 05:50:38|8835572673210010|azuser3544_mml.local@techademy.com|OPTIMIZE |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                      |NULL|{3519917694817942}|0611-043338-tjd8m6e5|4          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 6223, p25FileSize -> 3329, numDeletionVectorsRemoved -> 1, conflictDetectionTimeMs -> 13, minFileSize -> 3329, numAddedFiles -> 1, maxFileSize -> 3329, p75FileSize -> 3329, p50FileSize -> 3329, numAddedBytes -> 3329}                                                                                                                                                                                                                                                                                                                                                                                                                 |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|5      |2025-06-19 05:50:36|8835572673210010|azuser3544_mml.local@techademy.com|DELETE   |{predicate -> [\"(TripDurationMinutes#29469 > 60.0)\"]}                                                                                                                               |NULL|{3519917694817942}|0611-043338-tjd8m6e5|4          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 172, numDeletionVectorsUpdated -> 0, numDeletedRows -> 0, scanTimeMs -> 171, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0}                                                                                                                                                                                                                                                                                                                                                               |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|4      |2025-06-19 05:50:35|8835572673210010|azuser3544_mml.local@techademy.com|MERGE    |{predicate -> [\"(LogID#29460 = LogID#14593)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> []}|NULL|{3519917694817942}|0611-043338-tjd8m6e5|3          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 2901, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 1753, materializeSourceTimeMs -> 156, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 581, numTargetRowsUpdated -> 1, numOutputRows -> 1, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 1, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 996}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|3      |2025-06-19 05:50:32|8835572673210010|azuser3544_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                        |NULL|{3519917694817942}|0611-043338-tjd8m6e5|2          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 3322}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|2      |2025-06-19 05:50:07|8835572673210010|azuser3544_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                        |NULL|{3519917694817942}|0611-043338-tjd8m6e5|1          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 3322}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|1      |2025-06-19 05:46:29|8835572673210010|azuser3544_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                        |NULL|{3519917694817942}|0611-043338-tjd8m6e5|0          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 3322}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|0      |2025-06-19 05:46:14|8835572673210010|azuser3544_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                        |NULL|{3519917694817942}|0611-043338-tjd8m6e5|NULL       |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 3322}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "+-------+-------------------+----------------+----------------------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n",
      "\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+--------+\n",
      "|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|TripTime|\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+--------+\n",
      "| L001|     V001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|               19.0|          0|    19.0|\n",
      "| L002|     V002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|     100|               35.0|          0|    35.0|\n",
      "| L003|     V003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|               18.0|          0|    18.0|\n",
      "| L004|     V004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|               20.0|          1|    20.0|\n",
      "| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|               35.0|          0|    35.0|\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q8_DeltaLakeFeatures\n",
    "\n",
    "from pyspark.sql.functions import col, expr\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Save as Delta\n",
    "traffic_log_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/Workspace/Shared/traffic_delta\")\n",
    "\n",
    "# Load Delta table\n",
    "traffic_delta = DeltaTable.forPath(spark, \"dbfs:/Workspace/Shared/traffic_delta\")\n",
    "\n",
    "# Merge: Update tolls for Bikes (+10)\n",
    "traffic_delta.alias(\"old\").merge(\n",
    "    traffic_log_df.filter(col(\"VehicleType\") == \"Bike\").withColumn(\"TollPaid\", expr(\"TollPaid + 10\")).alias(\"new\"),\n",
    "    \"old.LogID = new.LogID\"\n",
    ").whenMatchedUpdate(set={\"TollPaid\": \"new.TollPaid\"}).execute()\n",
    "\n",
    "# Delete trips > 60 minutes\n",
    "traffic_delta.delete(\"TripDurationMinutes > 60\")\n",
    "\n",
    "# Describe history programmatically\n",
    "traffic_delta.history().show(truncate=False)\n",
    "\n",
    "# Read version 0\n",
    "df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"dbfs:/Workspace/Shared/traffic_delta\")\n",
    "df_v0.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af2ee610-7fd0-4a35-82f5-8390fab283d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+--------+--------+\n",
      "|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|TripTime|TripType|\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+--------+--------+\n",
      "| L001|     V001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|               19.0|          0|    19.0|  Medium|\n",
      "| L002|     V002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|     100|               35.0|          0|    35.0|    Long|\n",
      "| L003|     V003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|               18.0|          0|    18.0|  Medium|\n",
      "| L004|     V004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|               20.0|          1|    20.0|  Medium|\n",
      "| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|               35.0|          0|    35.0|    Long|\n",
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+--------+--------+\n",
      "\n",
      "+---------+----------+---------+----------------+\n",
      "|VehicleID|  TripDate|TripCount|IsMoreThan3Trips|\n",
      "+---------+----------+---------+----------------+\n",
      "|     V005|2024-05-01|        1|               0|\n",
      "|     V001|2024-05-01|        1|               0|\n",
      "|     V002|2024-05-01|        1|               0|\n",
      "|     V004|2024-05-01|        1|               0|\n",
      "|     V003|2024-05-01|        1|               0|\n",
      "+---------+----------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced Conditions\n",
    "# when/otherwise : Tag trip type as:\n",
    "# \"Short\" <15min\n",
    "# \"Medium\" 15-30min\n",
    "# \"Long\" >30min\n",
    "traffic_log_df_t2=traffic_log_df.withColumn(\"TripType\",when(col(\"TripDurationMinutes\")<15,\"Short\").when((col(\"TripDurationMinutes\")>=15) & (col(\"TripDurationMinutes\")<=30),\"Medium\").otherwise(\"Long\"))\n",
    "traffic_log_df_t2.show()\n",
    "# Flag vehicles with more than 3 trips in a day\n",
    "traffic_log_df_t=traffic_log_df.withColumn(\"TripDate\",date_format(\"EntryTime\",\"yyyy-MM-dd\"))\n",
    "traffic_log_df_t=traffic_log_df_t.groupBy(\"VehicleID\",\"TripDate\").count().withColumnRenamed(\"count\",\"TripCount\")\n",
    "traffic_log_df_t=traffic_log_df_t.withColumn(\"IsMoreThan3Trips\",when(col(\"TripCount\")>3,1).otherwise(0))\n",
    "traffic_log_df_t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc16e47-ebb4-447b-9f1e-89a47be193ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LogID: string (nullable = true)\n",
      " |-- VehicleID: string (nullable = true)\n",
      " |-- EntryPoint: string (nullable = true)\n",
      " |-- ExitPoint: string (nullable = true)\n",
      " |-- EntryTime: timestamp (nullable = true)\n",
      " |-- ExitTime: timestamp (nullable = true)\n",
      " |-- VehicleType: string (nullable = true)\n",
      " |-- SpeedKMH: integer (nullable = true)\n",
      " |-- TollPaid: integer (nullable = true)\n",
      " |-- TripDurationMinutes: double (nullable = true)\n",
      " |-- IsOverspeed: integer (nullable = false)\n",
      " |-- TripTime: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export & Reporting\n",
    "# Write final enriched DataFrame to:\n",
    "# Parquet partitioned by VehicleType\n",
    "traffic_log_df.printSchema()\n",
    "\n",
    "traffic_log_df.write.mode('overwrite').partitionBy(\"VehicleType\").format(\"parquet\").mode(\"overwrite\").save(\"dbfs:/FileStore/shared_uploads/traffic_logs_enriched\")\n",
    "# CSV for dashboards\n",
    "traffic_log_df.write.mode('overwrite').format(\"csv\").mode(\"overwrite\").save(\"dbfs:/FileStore/shared_uploads/traffic_logs_summary\")\n",
    "# Create summary SQL View: total toll by VehicleType + ExitPoint\n",
    "spark.sql(\"CREATE OR REPLACE TEMP VIEW traffic_logs_summary AS SELECT VehicleType, ExitPoint, sum(TollPaid) AS TotalTollPaid FROM traffic_logs_delta GROUP BY VehicleType, ExitPoint\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "june19assignment1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
