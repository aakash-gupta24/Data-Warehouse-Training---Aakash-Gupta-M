{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c27a6127-5ad1-4694-919a-472da7d3dea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=833800893595081#setting/sparkui/0611-043338-tjd8m6e5/driver-7664383072037392219\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ce4bdf0d590>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"June17Assignment3\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478dab61-ae19-4fbe-ae89-2d7b9f28103b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+\n",
      "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+\n",
      "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|\n",
      "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|\n",
      "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|\n",
      "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|\n",
      "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|\n",
      "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+\n",
      "\n",
      "+----------+--------+\n",
      "|Department|DeptHead|\n",
      "+----------+--------+\n",
      "|        IT|   Anand|\n",
      "|        HR|  Shruti|\n",
      "|   Finance|   Kamal|\n",
      "+----------+--------+\n",
      "\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+\n",
      "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|Weekday|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+\n",
      "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|    Wed|\n",
      "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|    Wed|\n",
      "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|    Thu|\n",
      "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|    Fri|\n",
      "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|    Fri|\n",
      "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|    Sat|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data Ingestion & Schema Handling\n",
    "# 1. Load the CSV using inferred schema.\n",
    "spark.conf.set(\"fs.azure.account.key.hestore.blob.core.windows.net\",\"------------AccessKeyyy---------------------\")\n",
    "\n",
    "employee_timesheet_df=spark.read.csv(\"wasbs://june17assignment3@hestore.blob.core.windows.net/employee_timesheet.csv\",header=True,inferSchema=True)\n",
    "department_location_df=spark.read.csv(\"wasbs://june17assignment3@hestore.blob.core.windows.net/department_location.csv\",header=True,inferSchema=True)\n",
    "\n",
    "employee_timesheet_df.show()\n",
    "department_location_df.show()\n",
    "# 2. Load the same file with schema explicitly defined.\n",
    "from pyspark.sql.types import *\n",
    "employee_timesheet_schema=StructType([StructField(\"EmployeeId\",IntegerType(),True),\n",
    "                                      StructField(\"Name\",StringType(),True),\n",
    "                                      StructField(\"Department\",StringType(),True),\n",
    "                                      StructField(\"Project\",StringType(),True),\n",
    "                                      StructField(\"WorkDate\",DateType(),True),\n",
    "                                      StructField(\"Location\",StringType(),True),\n",
    "                                      StructField(\"Mode\",StringType(),True)])\n",
    "department_location_schema=StructType([StructField(\"Department\",StringType(),True),\n",
    "                                      StructField(\"Location\",StringType(),True)])\n",
    "# 3. Add a new column Weekday extracted from WorkDate .\n",
    "from pyspark.sql.functions import *\n",
    "employee_timesheet_df=employee_timesheet_df.withColumn(\"Weekday\",date_format(employee_timesheet_df.WorkDate,\"E\"))\n",
    "employee_timesheet_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa6e6842-c956-4d2c-8c66-9b4844f9073f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|EmployeeId|TotalHours|\n",
      "+----------+----------+\n",
      "|      E103|         5|\n",
      "|      E104|         6|\n",
      "|      E101|        17|\n",
      "|      E102|        15|\n",
      "+----------+----------+\n",
      "\n",
      "+----------+----------+\n",
      "|Department|TotalHours|\n",
      "+----------+----------+\n",
      "|        IT|        23|\n",
      "|        HR|        15|\n",
      "+----------+----------+\n",
      "\n",
      "+----------+----------+\n",
      "|EmployeeId|TotalHours|\n",
      "+----------+----------+\n",
      "|      E101|        17|\n",
      "|      E102|        15|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregations & Grouping\n",
    "# 4. Calculate total work hours by employee.\n",
    "employee_timesheet_df.groupBy(\"EmployeeId\").agg(sum(\"WorkHours\").alias(\"TotalHours\")).show()\n",
    "# 5. Calculate average work hours per department.\n",
    "employee_timesheet_df.groupBy(\"Department\").agg(sum(\"WorkHours\").alias(\"TotalHours\")).orderBy(desc(\"TotalHours\")).limit(2).show()\n",
    "# 6. Get top 2 employees by total hours using window function.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "w=Window.partitionBy().orderBy(desc(\"TotalHours\"))\n",
    "employee_timesheet_df.groupBy(\"EmployeeId\").agg(sum(\"WorkHours\").alias(\"TotalHours\")).orderBy(desc(\"TotalHours\")).limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c5217f-f196-4204-9d6e-eedd63669675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+-------+---------+----------+--------+------+-------+\n",
      "|EmployeeID|Name|Department|Project|WorkHours|  WorkDate|Location|  Mode|Weekday|\n",
      "+----------+----+----------+-------+---------+----------+--------+------+-------+\n",
      "|      E102| Raj|        HR|   Beta|        8|2024-05-04|  Mumbai|Remote|    Sat|\n",
      "+----------+----+----------+-------+---------+----------+--------+------+-------+\n",
      "\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+------------+\n",
      "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|Weekday|RunningTotal|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+------------+\n",
      "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|    Wed|           8|\n",
      "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|    Fri|          17|\n",
      "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|    Wed|           7|\n",
      "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|    Sat|          15|\n",
      "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|    Thu|           5|\n",
      "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|    Fri|           6|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Date Operations\n",
    "# 7. Filter entries where WorkDate falls on a weekend.\n",
    "employee_timesheet_df.filter(employee_timesheet_df.Weekday.isin([\"Sat\",\"Sun\"])).show()\n",
    "# 8. Calculate running total of hours per employee using window.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum,rank\n",
    "w=Window.partitionBy(\"EmployeeId\").orderBy(employee_timesheet_df.WorkDate)\n",
    "employee_timesheet_df.withColumn(\"RunningTotal\",sum(\"WorkHours\").over(w)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c04eb1b-b2f0-470e-9ba0-7f1fe0eda642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "| Name|DeptHead|\n",
      "+-----+--------+\n",
      "|Anita|   Anand|\n",
      "|  Raj|  Shruti|\n",
      "| John|   Kamal|\n",
      "|Anita|   Anand|\n",
      "|Meena|   Anand|\n",
      "|  Raj|  Shruti|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining DataFrames\n",
    "# 10. Join with timesheet data and list all employees with their DeptHead.\n",
    "employee_timesheet_df.join(department_location_df,employee_timesheet_df.Department==department_location_df.Department,\"inner\").select(employee_timesheet_df.Name,department_location_df.DeptHead).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d415509d-8777-4c70-a663-9c06f564034c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+-----+\n",
      "|EmployeeId|Project|  Mode|Hours|\n",
      "+----------+-------+------+-----+\n",
      "|      E103|  Alpha|Onsite| NULL|\n",
      "|      E103|  Alpha|Remote|    5|\n",
      "|      E102|   Beta|Onsite|    7|\n",
      "|      E102|   Beta|Remote|    8|\n",
      "|      E101|  Alpha|Onsite| NULL|\n",
      "|      E101|  Alpha|Remote|   17|\n",
      "|      E104|  Gamma|Onsite|    6|\n",
      "|      E104|  Gamma|Remote| NULL|\n",
      "+----------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pivot & Unpivot\n",
    "# 11. Pivot table: total hours per employee per project.\n",
    "pivot_df=employee_timesheet_df.groupBy(\"EmployeeId\",\"Project\").pivot(\"Mode\").sum(\"workHours\")\n",
    "# 12. Unpivot example: Convert mode-specific hours into rows.\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "unpivot_df = pivot_df.selectExpr(\n",
    "    \"EmployeeId\",\n",
    "    \"Project\",\n",
    "    \"stack(2, 'Onsite', Onsite, 'Remote', Remote) as (Mode, Hours)\"\n",
    ")\n",
    "\n",
    "unpivot_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae54093-32cb-4ba3-9d5e-bcbb5d355f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+----------------+\n",
      "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|Weekday|WorkloadCategory|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+----------------+\n",
      "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|    Wed|            Full|\n",
      "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|    Wed|         Partial|\n",
      "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|    Thu|         Partial|\n",
      "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|    Fri|            Full|\n",
      "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|    Fri|         Partial|\n",
      "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|    Sat|            Full|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+----------------+\n",
      "\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+----------------+\n",
      "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|Weekday|WorkloadCategory|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+----------------+\n",
      "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|    Wed|            Full|\n",
      "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|    Wed|         Partial|\n",
      "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|    Thu|         Partial|\n",
      "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|    Fri|            Full|\n",
      "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|    Fri|         Partial|\n",
      "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|    Sat|            Full|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF & Conditional Logic\n",
    "# 13. Create a UDF to classify work hours:\n",
    "\n",
    "def workload_tag(hours):\n",
    "    if hours >= 8: return \"Full\"\n",
    "    elif hours >= 4: return \"Partial\"\n",
    "    else: return \"Light\"\n",
    "workload_tag_udf = udf(workload_tag,StringType())\n",
    "employee_timesheet_df.withColumn(\"WorkloadCategory\",workload_tag_udf(employee_timesheet_df.WorkHours)).show()\n",
    "# 14. Add a column WorkloadCategory using this UDF.\n",
    "employee_timesheet_df.withColumn(\"WorkloadCategory\",workload_tag_udf(employee_timesheet_df.WorkHours)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "334267b5-e11e-4784-abb3-141bae6bae26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+\n",
      "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|Weekday|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+\n",
      "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|      |    Wed|\n",
      "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|    Wed|\n",
      "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|    Thu|\n",
      "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|    Fri|\n",
      "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|    Fri|\n",
      "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|    Sat|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+\n",
      "\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+\n",
      "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|Weekday|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+\n",
      "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|      |    Wed|\n",
      "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|    Wed|\n",
      "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|    Thu|\n",
      "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|    Fri|\n",
      "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|    Fri|\n",
      "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|    Sat|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+\n",
      "\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+\n",
      "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|Weekday|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+\n",
      "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|      |    Wed|\n",
      "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|    Wed|\n",
      "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|    Thu|\n",
      "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|    Fri|\n",
      "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|    Fri|\n",
      "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|    Sat|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nulls and Cleanup\n",
    "# 15. Introduce some nulls in Mode column.\n",
    "employee_timesheet_df=employee_timesheet_df.withColumn(\"Mode\",when(rand()<0.1,\"\").otherwise(employee_timesheet_df.Mode))\n",
    "employee_timesheet_df.show()\n",
    "# 16. Fill nulls with \"Not Provided\".\n",
    "employee_timesheet_df=employee_timesheet_df.fillna(\"Not Provided\",subset=[\"Mode\"])\n",
    "employee_timesheet_df.show()\n",
    "# 17. Drop rows where WorkHours < 4.\n",
    "employee_timesheet_df=employee_timesheet_df.filter(employee_timesheet_df.WorkHours>4)\n",
    "employee_timesheet_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31a3aec-c85a-487a-b1f4-5da9c8c6789c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+------------+\n",
      "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|Weekday|RemoteWorker|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+------------+\n",
      "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|      |    Wed|          No|\n",
      "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|    Wed|          No|\n",
      "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|    Thu|         Yes|\n",
      "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|    Fri|         Yes|\n",
      "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|    Fri|          No|\n",
      "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|    Sat|         Yes|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+------------+\n",
      "\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+------------+----------+\n",
      "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|Weekday|RemoteWorker|ExtraHours|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+------------+----------+\n",
      "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|      |    Wed|          No|         0|\n",
      "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|    Wed|          No|         0|\n",
      "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|    Thu|         Yes|         0|\n",
      "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|    Fri|         Yes|         1|\n",
      "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|    Fri|          No|         0|\n",
      "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|    Sat|         Yes|         0|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced Conditions\n",
    "# 18. Use when-otherwise to mark employees as \"Remote Worker\" if >80% entries are\n",
    "# Remote.\n",
    "employee_timesheet_df=employee_timesheet_df.withColumn(\"RemoteWorker\",when((employee_timesheet_df.Mode==\"Remote\") & (employee_timesheet_df.Mode==\"Remote\") & (employee_timesheet_df.Mode==\"Remote\") & (employee_timesheet_df.Mode==\"Remote\") & (employee_timesheet_df.Mode==\"Remote\"),\"Yes\").otherwise(\"No\"))\n",
    "employee_timesheet_df.show()\n",
    "# 19. Add a new column ExtraHours where hours > 8.\n",
    "employee_timesheet_df=employee_timesheet_df.withColumn(\"ExtraHours\",when(employee_timesheet_df.WorkHours>8,employee_timesheet_df.WorkHours-8).otherwise(0))\n",
    "employee_timesheet_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06d77a18-920c-47ca-a7e9-f7de5f0017ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+------------+----------+\n",
      "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|Weekday|RemoteWorker|ExtraHours|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+------------+----------+\n",
      "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|      |    Wed|          No|         0|\n",
      "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|    Wed|          No|         0|\n",
      "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|    Thu|         Yes|         0|\n",
      "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|    Fri|         Yes|         1|\n",
      "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|    Fri|          No|         0|\n",
      "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|    Sat|         Yes|         0|\n",
      "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|      |    Wed|          No|         0|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+------------+----------+\n",
      "\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+------------+----------+\n",
      "|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|Weekday|RemoteWorker|ExtraHours|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+------------+----------+\n",
      "|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|    Sat|         Yes|         0|\n",
      "|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|    Fri|          No|         0|\n",
      "|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|    Wed|          No|         0|\n",
      "|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|    Thu|         Yes|         0|\n",
      "|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|    Fri|         Yes|         1|\n",
      "|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|      |    Wed|          No|         0|\n",
      "+----------+-----+----------+-------+---------+----------+---------+------+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Union + Duplicate Handling\n",
    "# 20. Append a dummy timesheet for new interns using unionByName() .\n",
    "employee_timesheet_df=employee_timesheet_df.unionByName(employee_timesheet_df.limit(1))\n",
    "employee_timesheet_df.show()\n",
    "# 21. Remove duplicate rows based on all columns.\n",
    "employee_timesheet_df=employee_timesheet_df.dropDuplicates()\n",
    "employee_timesheet_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "June17Assignment3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
