{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08e23db6-10b4-4558-962d-427982227eb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=833800893595081#setting/sparkui/0611-043338-tjd8m6e5/driver-3668778895047702419\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x70d0639614d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"June17Assignment2\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c15f5817-4b0b-4a60-ab58-303349617da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TransactionID: string (nullable = true)\n",
      " |-- Customer: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- UnitPrice: integer (nullable = true)\n",
      " |-- TotalPrice: integer (nullable = true)\n",
      " |-- TransactionDate: date (nullable = true)\n",
      " |-- PaymentMode: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- TransactionID: integer (nullable = true)\n",
      " |-- Customer: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- TotalPrice: double (nullable = true)\n",
      " |-- TransactionDate: date (nullable = true)\n",
      " |-- PaymentMode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basics\n",
    "# 1. Load retail_data.csv into a PySpark DataFrame and display schema.\n",
    "spark.conf.set(\"fs.azure.account.key.hestore.blob.core.windows.net\",\"------------AccessKeyyy---------------------\")\n",
    "\n",
    "retail_data_df=spark.read.csv(\"wasbs://june17assignment2@hestore.blob.core.windows.net/retail_data.csv\",header=True,inferSchema=True)\n",
    "retail_data_df.printSchema()\n",
    "# 2. Infer schema as False â€” then manually cast columns.\n",
    "#TransactionID\tCustomer\tCity\tProduct\tCategory\tQuantity\tUnitPrice\tTotalPrice\tTransactionDate\tPaymentMode\n",
    "retail_data_df=spark.read.csv(\"wasbs://june17assignment2@hestore.blob.core.windows.net/retail_data.csv\",header=True,inferSchema=False)\n",
    "retail_data_df=retail_data_df.withColumn(\"TransactionID\",retail_data_df.TransactionID.cast(\"int\"))\n",
    "retail_data_df=retail_data_df.withColumn(\"Customer\",retail_data_df.Customer.cast(\"String\"))\n",
    "retail_data_df=retail_data_df.withColumn(\"City\",retail_data_df.City.cast(\"String\"))\n",
    "retail_data_df=retail_data_df.withColumn(\"Product\",retail_data_df.Product.cast(\"String\"))\n",
    "retail_data_df=retail_data_df.withColumn(\"Category\",retail_data_df.Category.cast(\"String\"))\n",
    "retail_data_df=retail_data_df.withColumn(\"Quantity\",retail_data_df.Quantity.cast(\"int\"))\n",
    "retail_data_df=retail_data_df.withColumn(\"UnitPrice\",retail_data_df.UnitPrice.cast(\"double\"))\n",
    "retail_data_df=retail_data_df.withColumn(\"TotalPrice\",retail_data_df.TotalPrice.cast(\"double\"))\n",
    "retail_data_df=retail_data_df.withColumn(\"TransactionDate\",retail_data_df.TransactionDate.cast(\"date\"))\n",
    "retail_data_df=retail_data_df.withColumn(\"PaymentMode\",retail_data_df.PaymentMode.cast(\"String\"))\n",
    "retail_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f029ef-1ec0-4867-8dc0-928184bf709d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n",
      "|TransactionID|Customer|     City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n",
      "|         NULL|     Ali|   Mumbai| Laptop|Electronics|       1|  70000.0|   70000.0|     2024-01-15|       Card|\n",
      "|         NULL|    Neha|Bangalore| Tablet|Electronics|       2|  30000.0|   60000.0|     2024-01-20|        UPI|\n",
      "|         NULL|   Karan|   Mumbai|  Phone|Electronics|       1|  50000.0|   50000.0|     2024-02-15|       Card|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n",
      "\n",
      "+---------+\n",
      "|     City|\n",
      "+---------+\n",
      "|Bangalore|\n",
      "|   Mumbai|\n",
      "|    Delhi|\n",
      "|Hyderabad|\n",
      "+---------+\n",
      "\n",
      "+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n",
      "|TransactionID|Customer| City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|\n",
      "+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n",
      "|         NULL|    Zoya|Delhi|  Chair|  Furniture|       4|   5000.0|   20000.0|     2024-02-12|       Card|\n",
      "|         NULL|   Farah|Delhi|  Mouse|Electronics|       3|   1000.0|    3000.0|     2024-02-18|       Cash|\n",
      "+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n",
      "\n",
      "+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n",
      "|TransactionID|Customer| City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|\n",
      "+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n",
      "|         NULL|    Zoya|Delhi|  Chair|  Furniture|       4|   5000.0|   20000.0|     2024-02-12|       Card|\n",
      "|         NULL|   Farah|Delhi|  Mouse|Electronics|       3|   1000.0|    3000.0|     2024-02-18|       Cash|\n",
      "+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration & Filtering\n",
    "# 3. Filter transactions where TotalPrice > 40000 .\n",
    "retail_data_df.filter(retail_data_df.TotalPrice>40000).show()\n",
    "# 4. Get unique cities from the dataset.\n",
    "retail_data_df.select(\"City\").distinct().show()\n",
    "# 5. Find all transactions from \"Delhi\" using .filter() and .where() .\n",
    "retail_data_df.filter(retail_data_df.City==\"Delhi\").show()\n",
    "retail_data_df.where(retail_data_df.City==\"Delhi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8384db46-0743-4061-aac9-d1c1b1b87640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+---------------+\n",
      "|TransactionID|Customer|     City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|DiscountedPrice|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+---------------+\n",
      "|         NULL|     Ali|   Mumbai| Laptop|Electronics|       1|  70000.0|   70000.0|     2024-01-15|       Card|        63000.0|\n",
      "|         NULL|    Neha|Bangalore| Tablet|Electronics|       2|  30000.0|   60000.0|     2024-01-20|        UPI|        54000.0|\n",
      "|         NULL|    Ravi|Hyderabad|   Desk|  Furniture|       1|  15000.0|   15000.0|     2024-02-10|Net Banking|        13500.0|\n",
      "|         NULL|    Zoya|    Delhi|  Chair|  Furniture|       4|   5000.0|   20000.0|     2024-02-12|       Card|        18000.0|\n",
      "|         NULL|   Karan|   Mumbai|  Phone|Electronics|       1|  50000.0|   50000.0|     2024-02-15|       Card|        45000.0|\n",
      "|         NULL|   Farah|    Delhi|  Mouse|Electronics|       3|   1000.0|    3000.0|     2024-02-18|       Cash|         2700.0|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+---------------+\n",
      "\n",
      "+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n",
      "|TransactionID|Customer|     City|Product|   Category|Quantity|TotalPrice|   TxnDate|PaymentMode|DiscountedPrice|\n",
      "+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n",
      "|         NULL|     Ali|   Mumbai| Laptop|Electronics|       1|   70000.0|2024-01-15|       Card|        63000.0|\n",
      "|         NULL|    Neha|Bangalore| Tablet|Electronics|       2|   60000.0|2024-01-20|        UPI|        54000.0|\n",
      "|         NULL|    Ravi|Hyderabad|   Desk|  Furniture|       1|   15000.0|2024-02-10|Net Banking|        13500.0|\n",
      "|         NULL|    Zoya|    Delhi|  Chair|  Furniture|       4|   20000.0|2024-02-12|       Card|        18000.0|\n",
      "|         NULL|   Karan|   Mumbai|  Phone|Electronics|       1|   50000.0|2024-02-15|       Card|        45000.0|\n",
      "|         NULL|   Farah|    Delhi|  Mouse|Electronics|       3|    3000.0|2024-02-18|       Cash|         2700.0|\n",
      "+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Manipulation\n",
    "# 6. Add a column DiscountedPrice = TotalPrice - 10%.\n",
    "retail_data_df=retail_data_df.withColumn(\"DiscountedPrice\",retail_data_df.TotalPrice*0.9)\n",
    "retail_data_df.show()\n",
    "# 7. Rename TransactionDate to TxnDate .\n",
    "retail_data_df=retail_data_df.withColumnRenamed(\"TransactionDate\",\"TxnDate\")\n",
    "# 8. Drop the column UnitPrice .\n",
    "retail_data_df=retail_data_df.drop(\"UnitPrice\")\n",
    "retail_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a72da829-0a36-462c-9d3c-9e5ee292d411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|     City|sum(TotalPrice)|\n",
      "+---------+---------------+\n",
      "|Bangalore|        60000.0|\n",
      "|   Mumbai|       120000.0|\n",
      "|    Delhi|        23000.0|\n",
      "|Hyderabad|        15000.0|\n",
      "+---------+---------------+\n",
      "\n",
      "+-----------+---------------+\n",
      "|   Category|avg(TotalPrice)|\n",
      "+-----------+---------------+\n",
      "|Electronics|        45750.0|\n",
      "|  Furniture|        17500.0|\n",
      "+-----------+---------------+\n",
      "\n",
      "+-----------+-----+\n",
      "|PaymentMode|count|\n",
      "+-----------+-----+\n",
      "|Net Banking|    1|\n",
      "|       Card|    3|\n",
      "|       Cash|    1|\n",
      "|        UPI|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregations\n",
    "# 9. Get total sales by city.\n",
    "retail_data_df.groupBy(\"City\").sum(\"TotalPrice\").show()\n",
    "# 10. Get average unit price by category.\n",
    "retail_data_df.groupBy(\"Category\").avg(\"TotalPrice\").show()\n",
    "# 11. Count of transactions grouped by PaymentMode.\n",
    "retail_data_df.groupBy(\"PaymentMode\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dfa955a-7230-45e2-9261-68df3c1f4f95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----+\n",
      "|TransactionID|Customer|     City|Product|   Category|Quantity|TotalPrice|   TxnDate|PaymentMode|DiscountedPrice|rank|\n",
      "+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----+\n",
      "|         NULL|    Neha|Bangalore| Tablet|Electronics|       2|   60000.0|2024-01-20|        UPI|        54000.0|   1|\n",
      "|         NULL|   Farah|    Delhi|  Mouse|Electronics|       3|    3000.0|2024-02-18|       Cash|         2700.0|   1|\n",
      "|         NULL|    Zoya|    Delhi|  Chair|  Furniture|       4|   20000.0|2024-02-12|       Card|        18000.0|   2|\n",
      "|         NULL|    Ravi|Hyderabad|   Desk|  Furniture|       1|   15000.0|2024-02-10|Net Banking|        13500.0|   1|\n",
      "|         NULL|   Karan|   Mumbai|  Phone|Electronics|       1|   50000.0|2024-02-15|       Card|        45000.0|   1|\n",
      "|         NULL|     Ali|   Mumbai| Laptop|Electronics|       1|   70000.0|2024-01-15|       Card|        63000.0|   2|\n",
      "+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----+\n",
      "\n",
      "+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----+--------+\n",
      "|TransactionID|Customer|     City|Product|   Category|Quantity|TotalPrice|   TxnDate|PaymentMode|DiscountedPrice|rank|prev_txn|\n",
      "+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----+--------+\n",
      "|         NULL|    Neha|Bangalore| Tablet|Electronics|       2|   60000.0|2024-01-20|        UPI|        54000.0|   1|    NULL|\n",
      "|         NULL|   Farah|    Delhi|  Mouse|Electronics|       3|    3000.0|2024-02-18|       Cash|         2700.0|   1|    NULL|\n",
      "|         NULL|    Zoya|    Delhi|  Chair|  Furniture|       4|   20000.0|2024-02-12|       Card|        18000.0|   2|  3000.0|\n",
      "|         NULL|    Ravi|Hyderabad|   Desk|  Furniture|       1|   15000.0|2024-02-10|Net Banking|        13500.0|   1|    NULL|\n",
      "|         NULL|   Karan|   Mumbai|  Phone|Electronics|       1|   50000.0|2024-02-15|       Card|        45000.0|   1|    NULL|\n",
      "|         NULL|     Ali|   Mumbai| Laptop|Electronics|       1|   70000.0|2024-01-15|       Card|        63000.0|   2| 50000.0|\n",
      "+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window Functions\n",
    "# 12. Use a window partitioned by City to rank transactions by TotalPrice .\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "windowSpec=Window.partitionBy(\"City\").orderBy(\"TotalPrice\")\n",
    "retail_data_df=retail_data_df.withColumn(\"rank\",rank().over(windowSpec))\n",
    "retail_data_df.show()\n",
    "# 13. Use lag function to get previous transaction amount per city.\n",
    "from pyspark.sql.functions import lag\n",
    "retail_data_df=retail_data_df.withColumn(\"prev_txn\",lag(\"TotalPrice\").over(windowSpec))\n",
    "retail_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3812e3e2-1393-41ab-914f-a7417324bb9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     City|Region|\n",
      "+---------+------+\n",
      "|   Mumbai|  West|\n",
      "|    Delhi| North|\n",
      "|Bangalore| South|\n",
      "|Hyderabad| South|\n",
      "+---------+------+\n",
      "\n",
      "+------+---------------+\n",
      "|Region|sum(TotalPrice)|\n",
      "+------+---------------+\n",
      "| South|        75000.0|\n",
      "|  West|       120000.0|\n",
      "| North|        23000.0|\n",
      "+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_region_df=spark.read.csv(\"wasbs://june17assignment2@hestore.blob.core.windows.net/city_region.csv\",header=True,inferSchema=True)\n",
    "city_region_df.show()\n",
    "# 15. Join with main DataFrame and group total sales by Region.\n",
    "retail_data_df=retail_data_df.join(city_region_df,retail_data_df.City==city_region_df.City,\"inner\")\n",
    "retail_data_df.groupBy(\"Region\").sum(\"TotalPrice\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac4aa7a4-75e7-45aa-a153-880194cf807b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n",
      "|TransactionID|Customer|     City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n",
      "|         NULL|     Ali|   Mumbai| Laptop|Electronics|       1|  70000.0|   70000.0|     2024-01-15|       Card|\n",
      "|         NULL|    Neha|Bangalore| Tablet|Electronics|       2|  30000.0|   60000.0|     2024-01-20|        UPI|\n",
      "|         NULL|    Ravi|Hyderabad|   Desk|  Furniture|       1|  15000.0|   15000.0|     2024-02-10|Net Banking|\n",
      "|         NULL|    Zoya|    Delhi|  Chair|  Furniture|       4|   5000.0|   20000.0|     2024-02-12|       Card|\n",
      "|         NULL|   Karan|   Mumbai|  Phone|Electronics|       1|  50000.0|   50000.0|     2024-02-15|       Card|\n",
      "|         NULL|   Farah|    Delhi|  Mouse|Electronics|       3|   1000.0|    3000.0|     2024-02-18|       Cash|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n",
      "\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n",
      "|TransactionID|Customer|     City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n",
      "|         NULL|     Ali|   Mumbai| Laptop|Electronics|       1|  70000.0|   70000.0|     2024-01-15|       Card|\n",
      "|         NULL|    Neha|Bangalore| Tablet|Electronics|       2|  30000.0|   60000.0|     2024-01-20|        UPI|\n",
      "|         NULL|    Ravi|Hyderabad|   Desk|  Furniture|       1|  15000.0|   15000.0|     2024-02-10|Net Banking|\n",
      "|         NULL|    Zoya|    Delhi|  Chair|  Furniture|       4|   5000.0|   20000.0|     2024-02-12|       Card|\n",
      "|         NULL|   Karan|   Mumbai|  Phone|Electronics|       1|  50000.0|   50000.0|     2024-02-15|       Card|\n",
      "|         NULL|   Farah|    Delhi|  Mouse|Electronics|       3|   1000.0|    3000.0|     2024-02-18|       Cash|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nulls and Data Cleaning\n",
    "# 16. Introduce some nulls and replace them with default values.\n",
    "#insert some null values in retail_data_df\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Insert nulls into 'City' where TransactionID is divisible by 5\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Step 1: Read fresh from CSV (optional if you've already done this)\n",
    "retail_data_df = spark.read.csv(\n",
    "    \"wasbs://june17assignment2@hestore.blob.core.windows.net/retail_data.csv\",\n",
    "    header=True,\n",
    "    inferSchema=False\n",
    ")\n",
    "\n",
    "# Step 2: Cast columns correctly\n",
    "retail_data_df = retail_data_df.select(\n",
    "    col(\"TransactionID\").cast(\"int\"),\n",
    "    col(\"Customer\").cast(\"string\"),\n",
    "    col(\"City\").cast(\"string\"),\n",
    "    col(\"Product\").cast(\"string\"),\n",
    "    col(\"Category\").cast(\"string\"),\n",
    "    col(\"Quantity\").cast(\"int\"),\n",
    "    col(\"UnitPrice\").cast(\"double\"),\n",
    "    col(\"TotalPrice\").cast(\"double\"),\n",
    "    col(\"TransactionDate\").cast(\"date\"),\n",
    "    col(\"PaymentMode\").cast(\"string\")\n",
    ")\n",
    "\n",
    "# Step 3: Insert nulls into City and TotalPrice\n",
    "retail_data_df = retail_data_df.withColumn(\n",
    "    \"City\",\n",
    "    when(col(\"TransactionID\") % 5 == 0, None).otherwise(col(\"City\"))\n",
    ")\n",
    "\n",
    "retail_data_df = retail_data_df.withColumn(\n",
    "    \"TotalPrice\",\n",
    "    when(col(\"TransactionID\") % 7 == 0, None).otherwise(col(\"TotalPrice\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Insert nulls into 'TotalPrice' where TransactionID is divisible by 7\n",
    "retail_data_df = retail_data_df.withColumn(\n",
    "    \"TotalPrice\",\n",
    "    when(col(\"TransactionID\") % 7 == 0, None).otherwise(col(\"TotalPrice\"))\n",
    ")\n",
    "\n",
    "#replace\n",
    "retail_data_df = retail_data_df.fillna({\n",
    "    \"City\": \"Unknown\",\n",
    "    \"TotalPrice\": 0.0\n",
    "})\n",
    "retail_data_df.show()\n",
    "# 17. Drop rows where Quantity is null.\n",
    "retail_data_df=retail_data_df.fillna(0,subset=[\"Quantity\"])\n",
    "# 18. Fill null PaymentMode with \"Unknown\".\n",
    "retail_data_df=retail_data_df.fillna(\"Unknown\",subset=[\"PaymentMode\"])\n",
    "retail_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9fd000d-9b12-488d-8292-0c75d780b904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+----------+\n",
      "|TransactionID|Customer|     City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|OrderLabel|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+----------+\n",
      "|         NULL|     Ali|   Mumbai| Laptop|Electronics|       1|  70000.0|   70000.0|     2024-01-15|       Card|      High|\n",
      "|         NULL|    Neha|Bangalore| Tablet|Electronics|       2|  30000.0|   60000.0|     2024-01-20|        UPI|      High|\n",
      "|         NULL|    Ravi|Hyderabad|   Desk|  Furniture|       1|  15000.0|   15000.0|     2024-02-10|Net Banking|       Low|\n",
      "|         NULL|    Zoya|    Delhi|  Chair|  Furniture|       4|   5000.0|   20000.0|     2024-02-12|       Card|       Low|\n",
      "|         NULL|   Karan|   Mumbai|  Phone|Electronics|       1|  50000.0|   50000.0|     2024-02-15|       Card|    Medium|\n",
      "|         NULL|   Farah|    Delhi|  Mouse|Electronics|       3|   1000.0|    3000.0|     2024-02-18|       Cash|       Low|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 19. Write a UDF to label orders:\n",
    "def label_order(amount):\n",
    "    if amount > 50000: return \"High\"\n",
    "    elif amount >= 30000: return \"Medium\"\n",
    "    else: return \"Low\"\n",
    "# Apply this to classify TotalPrice .\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "label_order_udf = udf(label_order, StringType())\n",
    "retail_data_df=retail_data_df.withColumn(\"OrderLabel\",label_order_udf(retail_data_df.TotalPrice))\n",
    "retail_data_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91336603-aa3e-47ff-aaa2-d291c01ccfe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+----------+----+-----+---+\n",
      "|TransactionID|Customer|     City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|OrderLabel|year|month|day|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+----------+----+-----+---+\n",
      "|         NULL|     Ali|   Mumbai| Laptop|Electronics|       1|  70000.0|   70000.0|     2024-01-15|       Card|      High|2024|    1| 15|\n",
      "|         NULL|    Neha|Bangalore| Tablet|Electronics|       2|  30000.0|   60000.0|     2024-01-20|        UPI|      High|2024|    1| 20|\n",
      "|         NULL|    Ravi|Hyderabad|   Desk|  Furniture|       1|  15000.0|   15000.0|     2024-02-10|Net Banking|       Low|2024|    2| 10|\n",
      "|         NULL|    Zoya|    Delhi|  Chair|  Furniture|       4|   5000.0|   20000.0|     2024-02-12|       Card|       Low|2024|    2| 12|\n",
      "|         NULL|   Karan|   Mumbai|  Phone|Electronics|       1|  50000.0|   50000.0|     2024-02-15|       Card|    Medium|2024|    2| 15|\n",
      "|         NULL|   Farah|    Delhi|  Mouse|Electronics|       3|   1000.0|    3000.0|     2024-02-18|       Cash|       Low|2024|    2| 18|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+----------+----+-----+---+\n",
      "\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+----------+----+-----+---+\n",
      "|TransactionID|Customer|     City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|OrderLabel|year|month|day|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+----------+----+-----+---+\n",
      "|         NULL|    Ravi|Hyderabad|   Desk|  Furniture|       1|  15000.0|   15000.0|     2024-02-10|Net Banking|       Low|2024|    2| 10|\n",
      "|         NULL|    Zoya|    Delhi|  Chair|  Furniture|       4|   5000.0|   20000.0|     2024-02-12|       Card|       Low|2024|    2| 12|\n",
      "|         NULL|   Karan|   Mumbai|  Phone|Electronics|       1|  50000.0|   50000.0|     2024-02-15|       Card|    Medium|2024|    2| 15|\n",
      "|         NULL|   Farah|    Delhi|  Mouse|Electronics|       3|   1000.0|    3000.0|     2024-02-18|       Cash|       Low|2024|    2| 18|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Date & Time\n",
    "# 20. Extract year, month, and day from TxnDate .\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "retail_data_df=retail_data_df.withColumn(\"year\",year(retail_data_df.TransactionDate))\n",
    "retail_data_df=retail_data_df.withColumn(\"month\",month(retail_data_df.TransactionDate))\n",
    "retail_data_df=retail_data_df.withColumn(\"day\",dayofmonth(retail_data_df.TransactionDate))\n",
    "retail_data_df.show()\n",
    "# 21. Filter transactions that happened in February.\n",
    "retail_data_df.filter(retail_data_df.month==2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce7cfa7-ce8a-48e7-af63-564002e680b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+----------+----+-----+---+\n",
      "|TransactionID|Customer|     City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|OrderLabel|year|month|day|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+----------+----+-----+---+\n",
      "|         NULL|    Neha|Bangalore| Tablet|Electronics|       2|  30000.0|   60000.0|     2024-01-20|        UPI|      High|2024|    1| 20|\n",
      "|         NULL|     Ali|   Mumbai| Laptop|Electronics|       1|  70000.0|   70000.0|     2024-01-15|       Card|      High|2024|    1| 15|\n",
      "|         NULL|   Karan|   Mumbai|  Phone|Electronics|       1|  50000.0|   50000.0|     2024-02-15|       Card|    Medium|2024|    2| 15|\n",
      "|         NULL|    Zoya|    Delhi|  Chair|  Furniture|       4|   5000.0|   20000.0|     2024-02-12|       Card|       Low|2024|    2| 12|\n",
      "|         NULL|   Farah|    Delhi|  Mouse|Electronics|       3|   1000.0|    3000.0|     2024-02-18|       Cash|       Low|2024|    2| 18|\n",
      "|         NULL|    Ravi|Hyderabad|   Desk|  Furniture|       1|  15000.0|   15000.0|     2024-02-10|Net Banking|       Low|2024|    2| 10|\n",
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Union & Duplicate Handling\n",
    "# 22. Duplicate the DataFrame using union() and remove duplicates.\n",
    "retail_data_df=retail_data_df.union(retail_data_df)\n",
    "retail_data_df=retail_data_df.dropDuplicates()\n",
    "retail_data_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "June17Assignment2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
