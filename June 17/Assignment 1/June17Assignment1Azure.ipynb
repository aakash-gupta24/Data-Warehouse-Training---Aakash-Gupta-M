{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abb6f9ca-fd6b-402a-8b1a-c0e09df2a18a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=833800893595081#setting/sparkui/0611-043338-tjd8m6e5/driver-1841269832202128172\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7a2b3cac1410>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"June17Assignment1\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc61756-edf5-449b-8986-534d4764c79d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------+----------+\n",
      "|CustomerID|CustomerName|Region|SignupDate|\n",
      "+----------+------------+------+----------+\n",
      "|      C001|        Amit| North|2023-11-12|\n",
      "|      C002|        Sara| South|2024-01-08|\n",
      "|      C003|        John|  West|2023-06-20|\n",
      "|      C004|       Priya|  East|2024-03-15|\n",
      "+----------+------------+------+----------+\n",
      "\n",
      "+-------+----------+---------+--------+-----+----------+---------+\n",
      "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n",
      "+-------+----------+---------+--------+-----+----------+---------+\n",
      "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n",
      "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n",
      "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n",
      "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n",
      "|   3005|      C004|    P1004|       3|10000|2024-05-05|  Pending|\n",
      "+-------+----------+---------+--------+-----+----------+---------+\n",
      "\n",
      "+---------+-----------+-----------+-----+------------+\n",
      "|ProductID|ProductName|   Category|Stock|ReorderLevel|\n",
      "+---------+-----------+-----------+-----+------------+\n",
      "|    P1001|     Laptop|Electronics|    5|           2|\n",
      "|    P1002|      Phone|Electronics|   10|           3|\n",
      "|    P1003|     Tablet|Electronics|    7|           2|\n",
      "|    P1004|   Keyboard|Accessories|   15|           5|\n",
      "+---------+-----------+-----------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Ingestion & Schema Handling\n",
    "# 1. Load the CSV using inferred schema.\n",
    "spark.conf.set(\"fs.azure.account.key.hestore.blob.core.windows.net\",\"------------AccessKeyyy---------------------\")\n",
    "\n",
    "\"wasbs://june16assignment1@hestore.blob.core.windows.net/course_enrollments.csv\"\n",
    "customer_df=spark.read.csv(\"wasbs://june17assignment1@hestore.blob.core.windows.net/customers.csv\",header=True,inferSchema=True)\n",
    "orders_df=spark.read.csv(\"wasbs://june17assignment1@hestore.blob.core.windows.net/orders.csv\",header=True,inferSchema=True)\n",
    "products_df=spark.read.csv(\"wasbs://june17assignment1@hestore.blob.core.windows.net/products.csv\",header=True,inferSchema=True)\n",
    "customer_df.show()\n",
    "orders_df.show()\n",
    "products_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1e62824-331a-470b-a887-9c70d949f7b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------+----------+\n",
      "|CustomerID|CustomerName|Region|SignupDate|\n",
      "+----------+------------+------+----------+\n",
      "|      NULL|        Amit| North|2023-11-12|\n",
      "|      NULL|        Sara| South|2024-01-08|\n",
      "|      NULL|        John|  West|2023-06-20|\n",
      "|      NULL|       Priya|  East|2024-03-15|\n",
      "+----------+------------+------+----------+\n",
      "\n",
      "+-------+----------+---------+--------+-------+----------+---------+\n",
      "|OrderID|CustomerID|ProductID|Quantity|  Price| OrderDate|   Status|\n",
      "+-------+----------+---------+--------+-------+----------+---------+\n",
      "|   3001|      NULL|     NULL|       1|75000.0|2024-05-01|Delivered|\n",
      "|   3002|      NULL|     NULL|       2|50000.0|2024-05-02| Returned|\n",
      "|   3003|      NULL|     NULL|       1|30000.0|2024-05-03|Delivered|\n",
      "|   3004|      NULL|     NULL|       1|50000.0|2024-05-04|Delivered|\n",
      "|   3005|      NULL|     NULL|       3|10000.0|2024-05-05|  Pending|\n",
      "+-------+----------+---------+--------+-------+----------+---------+\n",
      "\n",
      "+---------+-----------+-----------+-----+------------+\n",
      "|ProductID|ProductName|   Category|Stock|ReorderLevel|\n",
      "+---------+-----------+-----------+-----+------------+\n",
      "|     NULL|     Laptop|Electronics|    5|           2|\n",
      "|     NULL|      Phone|Electronics|   10|           3|\n",
      "|     NULL|     Tablet|Electronics|    7|           2|\n",
      "|     NULL|   Keyboard|Accessories|   15|           5|\n",
      "+---------+-----------+-----------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Load the same file with schema explicitly defined.\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DecimalType, DateType\n",
    "\n",
    "customer_schema=StructType([StructField(\"CustomerID\",IntegerType(),True),\n",
    "                            StructField(\"CustomerName\",StringType(),True),\n",
    "                            StructField(\"Region\",StringType(),True),\n",
    "                            StructField(\"SignupDate\",DateType(),True)])\n",
    "orders_schema=StructType([StructField(\"OrderID\",IntegerType(),True),\n",
    "                           StructField(\"CustomerID\",IntegerType(),True),\n",
    "                           StructField(\"ProductID\",IntegerType(),True),\n",
    "                           StructField(\"Quantity\",IntegerType(),True),\n",
    "                           StructField(\"Price\",FloatType(),True),\n",
    "                           StructField(\"OrderDate\",DateType(),True),\n",
    "                           StructField(\"Status\",StringType(),True)])\n",
    "products_schema=StructType([StructField(\"ProductID\",IntegerType(),True),\n",
    "                            StructField(\"ProductName\",StringType(),True),\n",
    "                            StructField(\"Category\",StringType(),True),\n",
    "                            StructField(\"Stock\",IntegerType(),True),\n",
    "                            StructField(\"ReorderLevel\",IntegerType(),True)])\n",
    "customer_df_temp=spark.read.csv(\"wasbs://june17assignment1@hestore.blob.core.windows.net/customers.csv\",header=True,schema=customer_schema)\n",
    "orders_df_temp=spark.read.csv(\"wasbs://june17assignment1@hestore.blob.core.windows.net/orders.csv\",header=True,schema=orders_schema)\n",
    "products_df_temp=spark.read.csv(\"wasbs://june17assignment1@hestore.blob.core.windows.net/products.csv\",header=True,schema=products_schema)\n",
    "customer_df_temp.show()\n",
    "orders_df_temp.show()\n",
    "products_df_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d28998-9e71-444a-9d25-3d06db994e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|ProductName|Revenue|\n",
      "+-----------+-------+\n",
      "|      Phone| 150000|\n",
      "|     Laptop|  75000|\n",
      "|     Tablet|  30000|\n",
      "|   Keyboard|  30000|\n",
      "+-----------+-------+\n",
      "\n",
      "+------+-------+\n",
      "|Region|Revenue|\n",
      "+------+-------+\n",
      "| South| 100000|\n",
      "|  East|  30000|\n",
      "|  West|  30000|\n",
      "| North| 125000|\n",
      "+------+-------+\n",
      "\n",
      "+-----------------+\n",
      "|num_affected_rows|\n",
      "+-----------------+\n",
      "|                1|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PySpark + Delta\n",
    "# 1. Ingest all 3 CSVs as Delta Tables.\n",
    "customer_df.write.mode('overwrite').format(\"delta\").saveAsTable(\"customer\")\n",
    "orders_df.write.mode('overwrite').format(\"delta\").saveAsTable(\"orders\")\n",
    "products_df.write.mode('overwrite').format(\"delta\").saveAsTable(\"products\")\n",
    "# 2. Write SQL to get the total revenue per Product.\n",
    "spark.sql(\"select ProductName,sum(Quantity*Price) as Revenue from orders join products on orders.ProductID=products.ProductID group by ProductName\").show()\n",
    "# 3. Join Orders + Customers to find revenue by Region.\n",
    "spark.sql(\"select Region,sum(Quantity*Price) as Revenue from orders join customer on orders.CustomerID=customer.CustomerID group by Region\").show()\n",
    "# 4. Update the Status of Pending orders to 'Cancelled'.\n",
    "spark.sql(\"update orders set Status='Cancelled' where Status='Pending'\").show()\n",
    "# 5. Merge a new return record into Orders.\n",
    "import datetime\n",
    "new_return_df = spark.createDataFrame(\n",
    "    [(9999, 1, 1, 1, 0.0,  datetime.date(2025, 6, 17), 'Returned')],\n",
    "    schema=orders_schema\n",
    ")\n",
    "\n",
    "\n",
    "new_return_df.createOrReplaceTempView(\"new_return\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO orders AS target\n",
    "USING new_return AS source\n",
    "ON target.OrderID = source.OrderID\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET *\n",
    "WHEN NOT MATCHED\n",
    "  THEN INSERT *\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11c0e9ed-b848-408b-93b5-50f47ca6b44d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DLT Pipeline\n",
    "# 6. Create raw → cleaned → aggregated tables:\n",
    "# Clean: Remove rows with NULLs\n",
    "spark.sql(\"create table if not exists customer_cleaned as select * from customer where CustomerID is not null and CustomerName is not null and Region is not null and SignupDate is not null\")\n",
    "# Aggregated: Total revenue per Category\n",
    "spark.sql(\"create table if not exists products_aggregated as select Category,sum(Quantity*Price) as Revenue from orders join products on orders.ProductID=products.ProductID group by Category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe64bfdd-92e0-4b7f-90bf-27d0af5cd701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+--------+-----+----------+---------+\n",
      "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n",
      "+-------+----------+---------+--------+-----+----------+---------+\n",
      "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n",
      "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n",
      "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n",
      "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n",
      "|   3005|      C004|    P1004|       3|10000|2024-05-05|  Pending|\n",
      "+-------+----------+---------+--------+-----+----------+---------+\n",
      "\n",
      "+-------+----------+---------+--------+-----+----------+---------+\n",
      "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n",
      "+-------+----------+---------+--------+-----+----------+---------+\n",
      "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n",
      "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n",
      "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n",
      "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n",
      "|   3005|      C004|    P1004|       3|10000|2024-05-05|  Pending|\n",
      "+-------+----------+---------+--------+-----+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time Travel\n",
    "# 7. View data before the Status update.\n",
    "spark.sql(\"select * from orders version as of 0\").show()\n",
    "# 8. Restore to an older version of the orders table.\n",
    "spark.sql(\"RESTORE TABLE orders TO VERSION AS OF 0\")\n",
    "spark.sql(\"select * from orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d949878-1f15-4705-8b3a-a62e9445b918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vacuum + Retention\n",
    "# 9. Run VACUUM after changing default retention.\n",
    "spark.sql(\"ALTER TABLE orders SET TBLPROPERTIES (delta.logRetentionDuration = '10 hours')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "833a9abf-94f6-441a-8806-15e9b79ef321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+--------+-----+---------+------+\n",
      "|OrderID|CustomerID|ProductID|Quantity|Price|OrderDate|Status|\n",
      "+-------+----------+---------+--------+-----+---------+------+\n",
      "+-------+----------+---------+--------+-----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Expectations\n",
    "# 10. Quantity > 0 , Price > 0 , OrderDate is not null\n",
    "spark.sql(\"select * from orders where Quantity <= 0 or Price <= 0 or OrderDate is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1569c3b-7087-444a-b186-0477056f9048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+--------+-----+----------+---------+---------+\n",
      "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|OrderType|\n",
      "+-------+----------+---------+--------+-----+----------+---------+---------+\n",
      "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|   Normal|\n",
      "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|   Return|\n",
      "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|   Normal|\n",
      "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|   Normal|\n",
      "|   3005|      C004|    P1004|       3|10000|2024-05-05|  Pending|   Normal|\n",
      "+-------+----------+---------+--------+-----+----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bonus\n",
    "# 11. Use when-otherwise to create a new column: OrderType = \"Return\" if Status ==\n",
    "# 'Returned'\n",
    "spark.sql(\"select *, CASE WHEN Status='Returned' THEN 'Return' ELSE 'Normal' END as OrderType from orders\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "June17Assignment1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
